<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Oribi Analytics -->
    <script type="application/javascript">
        (function(b,o,n,g,s,r,c){if(b[s])return;b[s]={};b[s].scriptToken="XzgzMjUyODI5Ng";b[s].callsQueue=[];b[s].api=function(){b[s].callsQueue.push(arguments);};r=o.createElement(n);c=o.getElementsByTagName(n)[0];r.async=1;r.src=g;r.id=s+n;c.parentNode.insertBefore(r,c);})(window,document,"script","https://cdn.oribi.io/XzgzMjUyODI5Ng/oribi.js","ORIBI");
    </script>
    <!-- End Oribi Analytics -->
    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
        'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-NLJ2P23');</script>
    <!-- End Google Tag Manager -->
    <meta charset="utf-8"/>
    <title>Luca Mingarelli</title>
    <link rel="icon" href="../Icons/stork.png" type="image/png">
    <style>
        .collapsible {background-color: #777;color: white;cursor: pointer;  padding: 2px;width: 100%;
            border: none;  text-align: left;  outline: none;  font-size: 15px; border-radius: 2px;}
        /*.active,*/
        .collapsible:hover {  background-color: #555;}
        .content {display: none;  overflow: hidden;  background-color: #DCDCDC;}
    </style>
</head>

<!--<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>-->
<script>window.MathJax = {tex: {tags: 'ams'}};</script>
<script id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<!--<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG"></script>-->

<link rel="stylesheet" href="prism/prism.css"> <!-- For code highlight -->
<script src="prism/prism.js"></script>         <!-- For code highlight -->


<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="../css/w3.css">
<link rel="stylesheet" href="../css/font.css">
<link rel="stylesheet" href="../css/responsiveiframe.css">
<script src="../js/style_preamble.js"></script>

<body class="w3-light-grey w3-content" style="max-width:2600px">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NLJ2P23"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- NavigationBar -->
<script src="js/Navbar.js"></script>
<!-- _____________ -->
<!-- PRE_CONTENT -->
<script src="js/pre_content.js"></script>
<!-- _____________ -->




<div class="w3-content w3-justify" style="max-width:800px">

    <h1 style="text-align: left;"><b>Copulae — Concepts and examples</b></h1>




    <h2 style="text-align: left;">Definitions and basic properties</h2>

    <div style="overflow-x: auto;">
<b>Definition 1 — Copula</b><blockquote><i> A \(d\)-dimensional copula is a multivariate cumulative distribution function
    \(C:[0,1]^d\rightarrow[0,1]\) with marginals which are uniformly distributed on the unit interval:

    \begin{equation}
    C(u_1, ..., u_d)=\mathbb{P}(U_1\le u_1, ..., U_d\le u_d),
    \end{equation}

    where \(U_j\sim\mathcal{U}[0,1], \ \forall j\).</i>
    </blockquote>
    </div>
    <br>

    <b>Definition 2 — Copula density</b><blockquote><i>
    Given a differentiable copula \(C\) the copula density \(c\) is defined by:
    \begin{equation}
    c(u_1, ..., u_d) = \frac{\partial^dC(u_1, ..., u_d)}{\partial u_1...\partial u_d}.
    \label{eq:copula_density}
    \end{equation}
</i></blockquote>

    It follows that \(C(u_1, ..., u_d)\) is monotonically
    increasing in each component \(u_i=C(1,...,u_i, ..., 1)\).
    Moreover, the copula function \(C\) is isotonic,
    that is \(u_j\le v_j\ \forall j \implies C(u_1, ..., u_d)\le C(v_1, ..., v_d)\).
    Finally, notice that (under mild regularity conditions) one has the following proposition.
<br><br>
    <div style="overflow-x: auto;">
    <b>Proposition 3</b><blockquote><i> Given a \(d\)-dimensional copula \(C\) one obtains
    the conditional cumulative distribution function
    \(F(\mathbf{u}|U_j=u_j)=\mathbb{P}(U_1\le u_1, ..., U_d\le u_d\ |\ U_j=u_j) \) as
    \begin{equation}
    F(\mathbf{u}|U_j=u_j)=\frac{\partial}{\partial u_j} C(u_1, ..., u_d).
    \label{eq:conditional_copula}
    \end{equation}
</i></blockquote></div>


    <br>
    In order to better understand the significance of <i>Definition 1</i>
    let us consider a two-dimensional random vector \((X_1, X_2)\)
    with \(X_j\sim F_j\) and denoting the joint CDF by \(F\).
    Applying the <i><a href="https://en.wikipedia.org/wiki/Probability_integral_transform">probability integral transform</a></i>
    one can find two marginals \(U_j=F_j(X_j)\)
    uniform on the unit interval so that one can write
    \begin{equation}
    F(x_1, x_2)=C(F_1(x_1), F_2(x_2)).
    \label{eq1}
    \end{equation}

    Therefore, a <i>copula</i> is the recipe needed to link
    the marginals to the joint distribution.


<hr class="w3-opacity">

    <h6>Exercise 1</h6>
    Show that \(U_j=F_j(X_j)\) implies \(U_j\sim\mathcal{U}[0,1]\).
    <br>
    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 1</button>
        <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
        Start by considering the CDF \(F_{U_j}\) of \(U_j\). Then:
        \begin{align*}
        F_{U_j}(u_j) &= \mathbb{P}(U_j\le u_j) \newline
        &= \mathbb{P}(F_j(X_j)\le u_j) \newline
        &= \mathbb{P}(X_j\le F_j^{-1}(u_j)) \newline
        &= F_j(F_j^{-1}(u_j)) = u_j.
        \end{align*}
Because \(F_j(X_j)\in [0, 1]\) one can see that \(F_{U_j}(u_j) = u_j\) is the CDF of a
        uniform distribution on the unit interval.
            Therefore \(U_j\sim\mathcal{U}[0,1]\).
        <p align="right"> ◻</p>
    </div>

    <h6>Exercise 2</h6>
    Show that equation \eqref{eq1} holds.

    <br>
    <button class="collapsible" style="margin-bottom: 10px; margin-top: 10px">Solution 2</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">

        Notice that
        \begin{align*}
        F(x_1, x_2) &= \mathbb{P}(X_1 \leq x_1, X_2 \leq x_2) \newline
        &= \mathbb{P}(U_1 \leq F_1(x_1), U_2 \leq F_2(x_2)) \newline
        &= C(F_1(x_1), F_2(x_2)).
        \end{align*}

        <p align="right"> ◻</p>
    </div>

    <h6>Exercise 3</h6>
    Consider two uniform random variables \(U_1\) and \(U_2\) with known differentiable copula \(C\).
    Assuming \(U_1=u_1\) is observed, prove \eqref{eq:conditional_copula} in <i>Proposition 3</i>
    for this \(d=2\) case.

    <br>
    <button class="collapsible" style="margin-bottom: 10px; margin-top: 10px">Solution 3</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ;
    padding: 20px; border: 2px solid #9cc5ff;overflow-x: auto;">

        We have:
        \begin{align*}
        \mathbb{P}(U_2\le u_2\ |\ U_1=u_1) &= \lim_{\epsilon\rightarrow 0}\frac{\mathbb{P}(U_2\le u_2,\ U_1\in\ ]u_1-\epsilon, u_1+\epsilon])}{\mathbb{P}(U_1\in\ ]u_1-\epsilon, u_1+\epsilon])} \newline
        &= \lim_{\epsilon\rightarrow 0}\frac{C(u_1+\epsilon,u_2)-C(u_1-\epsilon,u_2)}{2\epsilon} \newline
        &= \frac{\partial}{\partial u_1}C(u_1, u_2).
        \end{align*}

        <p align="right"> ◻</p>
    </div>

<hr class="w3-opacity">

    <br>
Equation \eqref{eq1} can be formalised into the following theorem.
    <br><br>
    <div style="overflow-x: auto;">
    <b>Theorem 4 — Sklar</b><blockquote><i> Given a \(d\)-dimensional cumulative distribution function \(F\), with marginals \(F_j\),
    there exists a copula \(C\) such that
    \begin{equation}
    F(x_1, ..., x_d) = C(F_1(x_1), ..., F_d(x_d)),
    \label{eq:sklar}
    \end{equation}
    for all \(x_j\in\mathbb{R}\). Similarly, given a copula \(C\)
    and a set of \(d\) univariate cumulative distribution functions labelled \(F_1, ..., F_d\),
    equation \eqref{eq:sklar} uniquely defines a joint multivariate
    cumulative distribution function \(F\) with marginals \(F_1, ..., F_d\).</i></blockquote></div>

    <br>
    One has the following corollaries.

    <br><br>
    <div style="overflow-x: auto;">
    <b>Corollary 4.1</b><blockquote><i> From Sklar's theorem it follows that given a joint
    cumulative distribution function \(F\) and its marginals, the copula is defined by
    \begin{equation}
    C(u_1, .., u_d) = F\left(F^{-1}_1(u_1), ..., F^{-1}_d(u_d)\right),
    \end{equation}
    where \(F^{-1}_j(u_j) = \inf\{x: F_j(x)\ge u_j\}\).
</i></blockquote></div>
    <br>
    <div style="overflow-x: auto;">
    <b>Corollary 4.2</b><blockquote><i> From Sklar's theorem it follows that given the
    marginal cumulative distribution functions \(F_i\) and the associated
    marginal probability density functions \(f_i\),
    the joint probability density function \(f\) can be written as
    \begin{equation}
    f(x_1, .., x_d) = c\left(F_1(x_1), ..., F_d(x_d)\right)\prod_{i=1}^df_i(x_i),
    \label{eq:density_from_sklar}
    \end{equation}
    where \(c\) is the copula density defined in \eqref{eq:copula_density}.
</i></blockquote></div>


    <h3 style="text-align: left;">Fundamental copulae</h3>

    Let us now consider three fundamental examples corresponding to the cases of
    perfect dependence and independence. As we shall discuss,
    these are extreme cases which provide bounds to any other copula, and constitute therefore
    preliminary definitions for some results which will be presented later.
    First we consider the case of independent random variables.
<br><br>
    <b>Definition 5 — Independence copula</b><blockquote><i>
    The joint distribution of \(d\) independent random variables is defined by the
    independence copula
    \begin{equation}
    C_{\text{ind}}(u_1, ..., u_d) = \prod_{i=1}^d u_i.
    \label{eq:independence_copula}
    \end{equation}
</i></blockquote>
When all the random variables considered are deterministically related \(X_j = T_{ji}(X_i)\)
    for some set of strictly increasing transformations \(T_{ji}=T_{ij}^{-1}\), one can derive a relation between each pair
    of cumulative distribution functions as
    <div style="overflow-x: auto;">
    \[F_i(x) = \mathbb{P}(X_i\le x)
             = \mathbb{P}(T_{ji}(X_i)\le T_{ji}(x))
             = \mathbb{P}(X_j\le T_{ji}(x))
             = F_j(T_{ji}(x)),\]
    </div>
    which for all \(i,j\) implies
    \[U_i = F_i(X_i) = F_j(T_{ji}(X_i)) = F_j(X_j) = U_j.\]

    Similarly, consider two random variables such that
    \(X_2=D(X_1)\); for any strictly decreasing transformation \(D\) one has
    <div style="overflow-x: auto;">
    \[F_1(x) = \mathbb{P}(X_1 \leq x) = \mathbb{P}(D(X_1) \geq D(x))
    = \mathbb{P}(X_2 \geq D(x)) = 1 - F_2(D(x)),\]
    </div>
    which implies
    <div style="overflow-x: auto;">
    \[U_1 = F_1(X_1) = 1-F_2(D(X_1)) = 1-F_2(X_2) = 1-U_2.\]
    </div>

    This motivates the definition of the following <i>comonotonicity</i> and <i>counter-monotonicity</i> copulae.
    <br><br>
    <div style="overflow-x: auto;">
    <b>Definition 6 — Comonotonicity copula</b><blockquote><i>
    The copula associated with \(d\) equal uniformly distribute variables \(U_i=U_j,\ \forall\ i,j\),
    is called a comonotonicity copula and takes the form
    \begin{equation}
    C_{\text{co}}(u_1, ..., u_d) = \min\{u_1, ..., u_d\}.
    \label{eq:comonotonicity_copula}
    \end{equation}
</i></blockquote></div>
    <div style="overflow-x: auto;">
    <b>Definition 7 — Counter-monotonicity function</b><blockquote><i>
    The copula associated with \(d=2\) uniformly distributed random variables \(U_2=1-U_1\)
    is called a counter-monotonicity copula as takes the form
    \begin{equation}
    W_{\text{counter}}(u_1, u_2) = \max\{u_1 + u_2 -1, 0\}.
    \label{eq:counter_monotonicity_copula}
    \end{equation}
    A generalisation of \eqref{eq:counter_monotonicity_copula} to arbitrary \(d\) is given
    by the counter-monotonicity function
    \begin{equation}
    W_{\text{counter}}(u_1, ..., u_d) = \max\{\sum_{i=1}^d u_i - d + 1, 0\};
    \label{eq:counter_monotonicity_function}
    \end{equation}
    notice however that \eqref{eq:counter_monotonicity_function} is not a copula.

</i></blockquote></div>
    <br>


    <hr class="w3-opacity">
    <h6>Exercise 4</h6>
    Show that given two independent random variables \(X_1\) and \(X_2\)
    the associated joint multivariate cumulative distribution function takes
    indeed the form \eqref{eq:independence_copula} from <i>Definition 3</i>.
    <br>
    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 4</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
        The joint multivariate cumulative distribution funtion is \(F(x_1, x_2) = C(F_1(x_1), F_2(x_2))\). The independence of the two random variables \(X_1\perp\!\!\!\perp X_2\)
        also implies the associated uniform random variables \(U_j=F_j(X_j)\) are also independent:
        \(U_1 \perp\!\!\!\perp U_2\).
        Therefore, one has:
        \begin{align*}
        C(u_1, u_2) &= \mathbb{P}(U_1 \leq u_1, U_2 \leq u_2) \newline
                    &= \mathbb{P}(U_1 \leq u_1) \mathbb{P}(U_2 \leq u_2) \newline
                    &= u_1 u_2.
        \end{align*}
        Hence, \(F(x_1, x_2) = F_1(x_1)F_2(x_2)\).
        <p align="right"> ◻</p>
    </div>

    <h6>Exercise 5</h6>
    Show that <i>Definition 6</i> indeed is the copula associated with perfectly correlated random variables
    \(U_1=U_2\) for the case \(d=2\).
    <br>
    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 5</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
        Notice that
        \begin{align*}
        C(u_1, u_2) &= \mathbb{P}(U_1 \leq u_1, U_2 \leq u_2) \newline
        &= \mathbb{P}(U_1 \leq u_1, U_1 \leq u_2) \newline
        &= \mathbb{P}(U_1 \leq \min \{u_1, u_2\}) \newline
        &= \min \{u_1, u_2\}.
        \end{align*}
        <p align="right"> ◻</p>
    </div>


    <h6>Exercise 6</h6>
    Show that \eqref{eq:counter_monotonicity_copula} in <i>Definition 7</i> is
    indeed the copula associated with two random variables related by \(U_2=1-U_1\).
    <br>
    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 6</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
        Notice that
        \begin{align*}
        C(u_1, u_2) &= \mathbb{P}(U_1 \leq u_1, U_2 \leq u_2) \newline
        &= \mathbb{P}(U_1 \leq u_1, 1-U_1 \leq u_2) \newline
        &= \mathbb{P}(1 - u_2 \leq U_1 \leq u_1) \newline
        &= \max \{u_1 + u_2 - 1, 0\}.
        \end{align*}
        <p align="right"> ◻</p>
    </div>



    <hr class="w3-opacity">


As mentioned at the beginning of this section, these are extreme cases which
    are fundamental as they provide bounds to any other copula. This concept is formalised
    by the following theorem.

    <br><br>
    <b>Theorem 8 — Fréchet-Hoeffding bounds</b><blockquote><i> Given a copula
    \(C(\mathbf{u}) = C(u_1,..., u_d)\)
    one always has
    \begin{equation}
    W_{\text{counter}}(\mathbf{u})\le C(\mathbf{u}) \le C_{\text{co}}(\mathbf{u}).
    \label{eq:Frechet_Hoeffding_bounds}
    \end{equation}
</i></blockquote>

    <figure>
        <img class="center" alt="copulae_grid"
             src="img/Copulae/co_ind_counter.svg"
             width="80%" >
        <figcaption><i><b>Fig.1:</b> The bivariate countermonotonicity, independence, and comonotonicity copulae.
        All copulae exist as intermediate cases between these three.</i></figcaption>
    </figure>
Finally, let us consider the following proposition.

    <br><br>
    <b>Proposition 9</b><blockquote><i>
    Consider a random vector \(\mathbf{X}\) with copula \(C\),
    and let  \(T_1, ..., T_d\) be a set of \(d\) strictly increasing transformations.
    Then, the copula of the transformed vector \((T_1(X_1), ..., T_d(X_d))\) is also \(C\).

</i></blockquote>
    <br><br>


    <h2 style="text-align: left;">Parametric families of copulae</h2>
The following table provides some examples of notable parametric families.
    <br><br>
    <div style="overflow-x: auto;">
    <table class="w3-table-all w3-hoverable">
        <tr>
            <th>Family</th>
            <th>CDF</th>
            <th>Parameter</th>
        </tr>
        <tr>
            <th colspan="4" style="text-align:center;">Elliptical copulae</th>
        </tr>
        <tr>
            <td>Gaussian</td>
            <td>\(\Phi_2(\Phi^{-1}(u_1),\ \Phi^{-1}(u_2);\ \rho)\)</td>
            <td>\(\rho \in [0,1] \)</td>
        </tr>
        <tr>
            <td>Student-t</td>
            <td>\(T_{2,\nu}\left(T_{1,\nu}^{-1}(u_1),\ T_{1,\nu}^{-1}(u_2);\ \rho\right)\)</td>
            <td>\(\rho \in [0,1],\ \nu\ge1 \)</td>
        </tr>
        <tr>
            <th colspan="4" style="text-align:center;">Archimedean copulae</th>
        </tr>
        <tr>
            <td>Clayton</td>
            <td>\( \max\left\{u_1^{-\theta}+u_2^{-\theta} - 1,\ 0\right\}^{-1/\theta} \)</td>
            <td>\(\theta \in [-1, \infty[ \backslash \{0\} \)</td>
        </tr>
        <tr>
            <td>Gumbel</td>
            <td>\( \exp \left( -\left( (-\log u_1)^{\theta} + (-\log u_2)^{\theta} \right)^{1/\theta} \right) \)</td>
            <td>\(\theta \in [1, \infty[ \)</td>
        </tr>
        <tr>
            <td>Joe</td>
            <td>\( 1 - \left( (1-u_1)^{\theta} + (1-u_2)^{\theta} - (1-u_1)^{\theta} (1-u_2)^{\theta} \right)^{1/\theta} \)</td>
            <td>\(\theta \in [1, \infty[ \)</td>
        </tr>
        <tr>
            <td>Frank</td>
            <td>\(-\theta^{-1} \log \left( \frac{1 - e^{-\theta} - (1 - e^{-\theta u_1}) (1 - e^{-\theta u_2})} {1-e^{-\theta}} \right) \)</td>
            <td>\(\theta\in\mathbb{R}\backslash \{0\}\)</td>
        </tr>

    </table>
    </div>
    <br>

We can recognise two main classes of copulae: elliptical and archimedean.
    <i>Elliptical</i> copulae are implicit copulae arising from
    elliptical distributions
    (such as the normal, Student-t, and mixtures of normal distribution)
    via Sklar's theorem. Elliptical copulae have the advantage of allowing for
    flexible modelling of pairwise dependency, are simple to sample from, and usually densities are known explicitely.
    On the other hand, the copula itself is not explicit, and is characterised by
    radial symmetry, which implies identical lower and upper tail behaviour,
    as it we shall discuss.
    <br><br>
<i>Archimedean</i> copulae instead are explicit copulae which can be written
    in terms of some monotone generator
    \(\psi:[0,\infty[\rightarrow [0,1]\) such that \(\psi(0)=1\), \(\psi(\infty)=0\).
The associated archimedean copula is then written as
    \begin{equation}
    C(u_1, ..., u_d) = \psi\left(\sum_{i=1}^d\psi^{[-1]}(u_i)\right),
    \end{equation}
with the pseudo-inverse \(\psi^{[-1]}(x)=\psi^{-1}(x)\mathbb{1}_{x\le\psi(0)}\).
As an example, the generators of the archimedean copulae in the table above are:
    \begin{align*}
    \psi_{\text{Clayton}}(x) &= \theta^{-1}(x^{-\theta}-1),\newline
    \psi_{\text{Gumbel}}(x) &= (-\log x)^\theta,\newline
    \psi_{\text{Joe}}(x) &= -\log(1-(1-x)^\theta),\newline
    \psi_{\text{Frank}}(x) &= -\log\frac{e^{\theta x}-1}{e^{-\theta}-1}.
    \end{align*}

The explicit nature of Archimedean copulae makes them particularly suitable for
    closed form calculations, allowing for many of their properties to be
    explicitely computed in term of the generator \(\psi\). Moreover they are not
    anymore restricted to radial symmetries.

    <figure>
        <img class="center" alt="copulae_grid"
             src="img/Copulae/grid_copulae.svg"
              width="80%" >
        <figcaption><i><b>Fig.2:</b> Bivariate probability density functions with normal marginals and selected copulae.</i></figcaption>
        <button class="collapsible">Plot code</button>
        <div class="content"><pre><code class="lang-python">
import numpy as np, matplotlib.pyplot as plt, seaborn as sns
import scipy.stats as st
sns.set()

n = 1000
x1 = x2 = np.linspace(-2.7, 2.7, n)
dx = x1[-1] - x2[-2]
u1, u2 = st.norm.cdf(x1), st.norm.cdf(x2)

# INDEPENDENCE COPULA
def Ind_cop(u1, u2):
    U1, U2 = np.meshgrid(u1, u2)
    return U1 * U2

# GAUSSIAN COPULA
def Gauss_cop(u1, u2, rho=0.5):
    """Rho in [0, 1]"""
    iU1, iU2 = np.meshgrid(st.norm.ppf(u1), st.norm.ppf(u2))
    jnt = st.multivariate_normal.cdf(np.array([iU1, iU2]).T,
                                     mean=[0, 0], cov=[[1, rho], [rho, 1]])
    return jnt

# CLAYTON COPULA
def Clayton_cop(u1, u2, theta=1.):
    """Theta in [-1, inf[ \ {0}"""
    U1, U2 = np.meshgrid(u1, u2)
    jnt = (U1**(-theta) + U2**(-theta) - 1).clip(min=0) ** (-1/theta)
    return jnt

# GUMBEL COPULA
def Gumbel_cop(u1, u2, theta=1.):
    """Theta in [1, inf["""
    U1, U2 = np.meshgrid(u1, u2)
    jnt = np.exp(-((-np.log(U1))**theta
                   +(-np.log(U2))**theta)**(1/theta))
    return jnt

# JOE COPULA
def Joe_cop(u1, u2, theta=1.):
    """Theta in [1, inf["""
    U1, U2 = np.meshgrid(u1, u2)
    jnt = 1 - ((1-U1)**theta + (1-U2)**theta - ((1-U1)*(1-U2))**theta) ** (1/theta)
    return jnt

# FRANK COPULA
def Frank_cop(u1, u2, theta=1.):
    """Theta in [-inf, inf[ \ {0}"""
    U1, U2 = np.meshgrid(u1, u2)
    jnt = -(1/theta) * np.log(1+((np.exp(-theta*U1) - 1)*(np.exp(-theta*U2) - 1))/(np.exp(-theta) - 1))
    return jnt

J = Clayton_cop(u1, u2, theta=5)
dJ = (np.diff(np.diff(J).T).T).clip(min=1e-10)
plt.contour(x1[1:], x2[1:], dJ, 10)
plt.show()
</code></pre>
        </div>
    </figure>





<br>
    <hr class="w3-opacity">

    <h6>Exercise 7</h6>
    Given a vector \((X_1, X_2)\) from the standard bivariate
    normal distribution with correlation \(\rho\), derive the gaussian copula
    presented in the table above.
    <br>
    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 7</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
        The associated copula is given by
        \begin{align*}
        C(u_1, u_2) &= \mathbb{P}(U_1 \leq u_1, U_2 \leq u_2) \newline
        &= \mathbb{P}(X_1 \leq \Phi^{-1}(u_1), X_2 \leq \Phi^{-1}(u_2)) \newline
        &= \Phi_2(\Phi^{-1}(u_1), \Phi^{-1}(u_2); \rho).
        \end{align*}
        <p align="right"> ◻</p>
    </div>

    <h6 id="EX8">Exercise 8</h6>
    Show that the limiting copula of the Clayton copula \(C_\theta^{\text{Clayton}}(u_1, u_2)\)
    for \(\theta\rightarrow\infty\) is the comonotonicity copula \(C_{\text{co}}\) and
    for \(\theta\rightarrow -1\) is the countermonotonicity copula \(W_{\text{counter}}\).
    Moreover show that for \(\theta\rightarrow 0\) Clayton's copula tends to the independence copula.
    <br>
    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 8</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ;
    padding: 20px; border: 2px solid #9cc5ff;overflow-x: auto;">
        The case \(\theta\rightarrow -1\) is trivial.

        <p align="right"> ◻</p>

        Let's now consider the case \(\theta\rightarrow \infty\).
Assume \(u_1 \le u_2 \) in \([0,1]\). Then
        \[
u_1^{-\theta} \le \max\left\{u_1^{-\theta}+u_2^{-\theta} - 1,\ 0\right\} \le 2u_1^{-\theta}.
        \]
This in turns implies
        \[
2^{-1/\theta}u_1\le C_\theta^{\text{Clayton}}(u_1, u_2) \le u_1.
        \]
        Hence, one has
        \[
\lim_{\theta\rightarrow \infty}C_\theta^{\text{Clayton}}(u_1, u_2) = \text{min}\{u_1, u_2\}
        =C_{\text{co}}
        \]

        <p align="right"> ◻</p>

        Let's now consider the case \(\theta\rightarrow 0\). For \(u_1, u_2 \in [0,1]\) one has
        \[
u_j^{-\theta} = e^{-\theta\log u_j} \xrightarrow[]{\theta\rightarrow 0} 1-\theta\log u_j +o(\theta).
        \]
Hence
        \begin{align*}
\log C_\theta^{\text{Clayton}}(u_1, u_2)
        &= \frac{1}{\theta}\log(1-\theta\log(u_1u_2) + o(\theta)) \newline
        &\xrightarrow[]{\theta\rightarrow 0}\log(u_1u_2)
        \end{align*}

        <p align="right"> ◻</p>

    </div>

    <hr class="w3-opacity">


<h2 style="text-align: left;">Simulating from Copulae</h2>

    Let us now consider the essential recipes for drawing
    uniform random vectors \((U_1, ..., U_d)\) with a desired copula and uniform marginals,
    which can be useful e.g. to perform Monte Carlo simulations. Clearly,
    arbitrary marginals can be obtained via the quantile transformation as dictated
    by the probability integral transform discussed earlier.

<h4>Implicit copulae</h4>
    For implicit copulae derived from a multivariate distribution such as
    the Gaussian and Student-t the procedure is quite straightforward.
    <br><br>
    <b>Algorithm 10 — Gaussian Copula</b><blockquote><i>
<ol>
    <li>Given a correlation matrix \(\Pi\) perform a Cholesky decomposition, that is,
        find \(\Lambda\) such that \(\ \Pi = \Lambda^T\Lambda\); </li>
    <li>Generate iid standard normals \(\mathbf{G}= (G_1, ..., G_d)^T\);</li>
    <li>Compute \(\ \mathbf{X} = \Lambda\mathbf{G}\);</li>
    <li>Return the desired vector of \(\ U_i = \Phi(X_i)\),
        with \(\Phi\) the standard normal cumulative distribution function.</li>
</ol>
</i></blockquote>
    <b>Algorithm 11 — Student-t Copula</b><blockquote><i>
    <ol>
        <li>Given a correlation matrix \(\ \Pi\), generate a normal vector \(\mathbf{X}\)
            performing steps (1) to (4) from Algorithm 10; </li>
        <li>Generate \(\xi=\sum_{i=1}^\nu Y_i^2\),
            with \(Y_i \stackrel{\text{iid}}{\sim} \mathcal{N}(0,1)\),
            so that \(\xi\stackrel{\text{iid}}{\sim}\chi_\nu^2\);</li>
        <li>Return the desired vector \(\ U_i = t_\nu(X_i/\sqrt{\xi/\nu})\),
            with \(t_\nu\) the univariate Student-t cumulative distribution function.</li>
    </ol>
</i></blockquote>

    <h4>Explicit copulae</h4>

    The Archimedean copulae we have considered are actually part of a class of
    copulae known as <i>Laplace Transform Archimedean Copulae</i> (LT-Archimedean for short)
    where the inverse of the generator \(\phi^{[-1]}\) can be represented as a Laplace
    transform of some function \(G\):
    \begin{equation}
    \phi^{[-1]}(x) = \int_0^\infty e^{-tx}\text{d}G(t).
    \label{eq:LT-A}
    \end{equation}
    More specifically, one can show <a href="#ref4">[4]</a> that all completely monotone
    Archimedean generators are Laplace-Stieltjes transforms
    of distribution functions on \(\mathbb{R}^+\).
With this in mind, we are now ready to consider an algorithm to sample from such
    copulae.
    <br><br>
    </i></blockquote>
    <b>Algorithm 12 — LT-Archimedean Copulae</b><blockquote><i>
    Given an LT-Archimedean copula with generator \(\phi\),
    <ol>
        <li>Find a CDF \(G\) which satisfies \eqref{eq:LT-A}; </li>
        <li>Generate a variate \(V\sim G\);</li>
        <li>Generate uniform iid \(X_1, ..., X_d\);</li>
        <li>Return the vector with elements
            \(U_i = \phi^{[-1]}\left(-\frac{\log X_i}{V}\right)\).</li>
    </ol>
    See <a href="#ref1">[1]</a> and <a href="#ref5">[5]</a> for more details.
</i></blockquote>

For the special case of \(d=2\) one has however a simpler approach.
    <br><br>
    </i></blockquote>
    <b>Algorithm 13 — 2D Explicit Copulae</b><blockquote><i>
    Consider the conditional distribution
    \(F_{1}=\mathbb{P}(U_1\le u_1 | U_2=u_2)\).
    Recalling \eqref{eq:conditional_copula} from Proposition 3,
    this is equivalent to
    \begin{equation*}
    F_{1} = c_{1}(u_1, u_2) = \frac{\partial}{\partial u_2} C(u_1, u_2).
    \end{equation*}
    Therefore one obtains the desired
    \((U_1,\ U_2)\) via the following algorithm:
    <ol>
        <li>Sample \(u_1, t \overset{\text{iid}}{\sim} \mathcal{U}[0,1]\); </li>
        <li>Compute \(u_2 = c_{1}^{[-1]}(u_1, t)\);</li>
    </ol>
</i></blockquote>
    <br><br>

    <h5>Example 1: gaussian copula and gaussian marginals</h5>
    The simplest example is that in which we sample
    from a gaussian joint distribution, since in most programming
    languages this is often well implemented in libraries.
    Fig.3 below presents an example where the marginals follow standard
    normal distributions and the gaussian copula has linear correlation
    \(\rho=0.75\).

    <figure>
        <img class="center" alt="copulae_grid"
             src="img/Copulae/joint_G.svg"
             width="60%" >
        <figcaption><i><b>Fig.3:</b> Joint distribution with gaussian copula
            and standard normal marginals.</i></figcaption>
        <button class="collapsible">Plot code</button>
        <div class="content"><pre><code class="lang-python">
import pandas as pd, matplotlib.pyplot as plt, seaborn as sns
import scipy.stats as st
N = 1_000_000
mvnorm = st.multivariate_normal(mean=[0, 0], cov=[[1., 0.75],
                                                  [0.75, 1.]])
X = pd.DataFrame(mvnorm.rvs(N), columns=['X1', 'X2'])

jnt_kw = dict(height=5, ratio=5, space=0,
              color="tab:blue", alpha=0)

h = (sns.jointplot(data=X, x='X1', y='X2', marginal_kws=dict(bins=30),
                   xlim=(-3,3), ylim=(-3,3), kind='hex', gridsize=2, **jnt_kw)
        .plot_joint(sns.kdeplot, shade=False, cmap='viridis'))
plt.show()</code></pre>
        </div>
    </figure>



    <h5>Example 2: gaussian copula and non-gaussian marginals</h5>
    In order to produce samples from a joint with gaussian copula
    but non-gaussian marginals we first need to obtaint the copula's
    uniform marginals \(U_i = \Phi(X_i)\).

    <figure>
        <img class="center" alt="copulae_grid"
             src="img/Copulae/hex_copula.svg"
             width="60%" >
        <figcaption><i><b>Fig.4:</b> Gaussian copula
            with its uniform marginals.</i></figcaption>
        <button class="collapsible">Plot code</button>
        <div class="content"><pre><code class="lang-python">
import pandas as pd, matplotlib.pyplot as plt, seaborn as sns
import scipy.stats as st
N = 1_000_000
mvnorm = st.multivariate_normal(mean=[0, 0], cov=[[1., 0.75],
                                                  [0.75, 1.]])
X = pd.DataFrame(mvnorm.rvs(N), columns=['X1', 'X2'])

jnt_kw = dict(height=5, ratio=5, space=0,
              color="tab:blue", alpha=0)

U = pd.DataFrame(st.norm().cdf(X), columns=['U2', 'U1'])

h = (sns.jointplot(data=U, x='U1', y='U2', marginal_kws=dict(bins=30),
                   xlim=(0,1), ylim=(0,1),kind='hex', gridsize=2, **jnt_kw)
        .plot_joint(plt.hexbin, gridsize=30, cmap='Blues'))
plt.show()</code></pre>
        </div>
    </figure>

    <br><br>
Finally, the copula's uniforms can be transformed to the desired marginals.
    Take as an example \(\widetilde{X}_1\sim \text{Beta}(\alpha=3, \beta=10)\),
    and \(\widetilde{X}_2\sim \text{Gumbel}(\mu=0, \beta=1)\): Figure 5 below
    plots the joint density with these marginals
    and gaussian copula with \(\rho=0.75\),
    while Figure 6, plots these two marginals on the independence copula.

    <figure>
        <img class="center" alt="copulae_grid"
             src="img/Copulae/Gcop_gumbBeta_marginals.svg"
             width="60%" >
        <figcaption><i><b>Fig.5:</b> Gaussian copula
            with \(\text{Beta}(\alpha=3, \beta=10)\) and
            \(\text{Gumbel}(\mu=0,\beta=1)\)
            marginals.</i></figcaption>
    </figure>
    <figure>
        <img class="center" alt="copulae_grid"
             src="img/Copulae/IndCop_gumbBeta_marginals.svg"
             width="60%" >
        <figcaption><i><b>Fig.6:</b> Independence copula
            with \(\text{Beta}(\alpha=3, \beta=10)\) and
            \(\text{Gumbel}(\mu=0,\beta=1)\)
            marginals.</i></figcaption>
        <button class="collapsible">Plot code</button>
        <div class="content"><pre><code class="lang-python">
import pandas as pd, matplotlib.pyplot as plt, seaborn as sns
import scipy.stats as st
N = 1_000_000
mvnorm = st.multivariate_normal(mean=[0, 0], cov=[[1., 0.75],
                                                  [0.75, 1.]])
X = pd.DataFrame(mvnorm.rvs(N), columns=['X1', 'X2'])

jnt_kw = dict(height=5, ratio=5, space=0,
              color="tab:blue", alpha=0)

U = pd.DataFrame(st.norm().cdf(X), columns=['U2', 'U1'])

# With Gaussian Copula
x1_t = st.beta(a=3, b=10).ppf(U.U1)
x2_t = st.gumbel_l().ppf(U.U2)
X_trans = pd.DataFrame([x1_t, x2_t], index=['XT1', 'XT2']).T

h = (sns.jointplot(data=X_trans, x='XT1', y='XT2', kind='hex', gridsize=2,
                   marginal_kws=dict(bins=30),
                   xlim=(0, 0.6), ylim=(-5, 2.0),
                   **jnt_kw)
        .plot_joint(sns.kdeplot, cmap='viridis'))
plt.show()

# With Independence Copula
x1, x2 = st.beta(a=3, b=10).rvs(N), st.gumbel_l().rvs(N)
h = (sns.jointplot(x=x1, y=x2, kind='hex', marginal_kws=dict(bins=30),
                   xlim=(0,0.6), ylim=(-5,2.0),
                   **jnt_kw)
        .plot_joint(sns.kdeplot, cmap='viridis'))
plt.show()</code></pre>
        </div>
    </figure>



    <h5>Example 3: non-gaussian copula and non-gaussian marginals</h5>
At last let's replicate the last exercise replacing the gaussian copula
    with e.g. a Clayton copula.
    One can find <a href="#ref5">[5]</a> the correct
    cumulative distribution function \(G\) to be considered in this case
    is that of a Gamma distribution \(\Gamma(\theta^{-1},1)\).
<!--    Recalling Algorithm 13, one can compute the conditional distribution-->
<!--    \begin{align*}-->
<!--c_{1}^{\text{Clayton}} &= \frac{\partial C^{\text{Clayton}}(u_1, u_2)}{\partial u_1} \newline-->
<!--    &=\frac{\partial }{\partial u_1} \text{max}\Big\{u_1^{-\theta}+u_2^{-\theta}-1, 0\Big\}^{-1/\theta} \newline-->
<!--    &= \frac{u_1^\theta(u_1^{-\theta}+u_2^{-\theta}-1)}{u_2(u_1^\theta u_2^\theta-u_1^\theta-u_2^\theta)} \mathbb{1}_{u_1^{-\theta}+u_2^{-\theta}-1>0}-->
<!--    \end{align*}-->

    <figure>
        <img class="center" alt="copulae_grid"
             src="img/Copulae/hex_Claytoncopula.svg"
             width="60%" >
        <figcaption><i><b>Fig.7:</b> Clayton copula
            with \(\theta=2\).</i></figcaption>
    </figure>
    <figure>
        <img class="center" alt="copulae_grid"
             src="img/Copulae/Claytoncop_gumbBeta_marginals.svg"
             width="60%" >
        <figcaption><i><b>Fig.8:</b> Clayton copula
            with \(\text{Beta}(\alpha=3, \beta=10)\) and
            \(\text{Gumbel}(\mu=0,\beta=1)\)
            marginals.</i></figcaption>
        <button class="collapsible">Plot code</button>
        <div class="content"><pre><code class="lang-python">
import pandas as pd, matplotlib.pyplot as plt, seaborn as sns
import scipy.stats as st

jnt_kw = dict(height=5, ratio=5, space=0,
              color="tab:blue", alpha=0)

N = 1_000_000
θ = 2
def clayton(θ, N, d=2):
    Γ = st.gamma.rvs(a=1/θ, scale=1, size=N)
    X = st.uniform.rvs(size=(d,N))
    return (1 - np.log(X) / Γ) ** (-1/θ)

U1, U2 = clayton(θ=θ, N=N, d=2)
U = pd.DataFrame([U1, U2], index=['U1', 'U2']).T

h = (sns.jointplot(data=U, x='U1', y='U2', marginal_kws=dict(bins=30),
                   xlim=(0,1), ylim=(0,1),
                   kind='hex', gridsize=2, **jnt_kw)
        .plot_joint(plt.hexbin, gridsize=30, cmap=my_cmap))
plt.show()


x1_t = st.beta(a=3, b=10).ppf(U.U1)
x2_t = st.gumbel_l().ppf(U.U2)
X_trans = pd.DataFrame([x1_t, x2_t], index=['XT1', 'XT2']).T

h = (sns.jointplot(data=X_trans, x='XT1', y='XT2', kind='hex', gridsize=2,
                   marginal_kws=dict(bins=30),
                   xlim=(0, 0.6), ylim=(-6, 2.0),
                   ** jnt_kw)
        .plot_joint(sns.kdeplot, cmap='viridis'))
plt.show()</code></pre>
        </div>
    </figure>


    <br><br>
In Python, the package
    <a href="https://openturns.github.io/openturns/latest/index.html"><i><b>OpenTURNS</b></i></a>
provides a fast implementation of copulas. As an example, one can perform sampling
    from the Clayton copula we have considered so far simply as it follows:
<pre><code class="lang-python">import openturns as ot, numpy as np

X = ot.ComposedDistribution([ot.Uniform(0, 1),
                             ot.Uniform(0, 1)],
                            ot.ClaytonCopula(2))

U1, U2 = np.array(X.getSample(1_000_000)).T</code></pre>

As usual the two uniform marginals can now be transformed into
    whichever marginals one might desire.








    <h2  style="text-align: left;">Estimation: Fitting Copulae to Data</h2>

    The task of fitting a copula to data is a rather challenging one.
Generally speaking, one approach could follow a Maximum Likelihood Estimation principle.
    For example, given the random vector  \(\mathbf{X}=(X_1, ..., X_d)\), and assuming
    parametric models for the copula density \(c_{\mathbf{X}}(\cdot|\mathbf{\theta}_C)\)
    and for the marginal cumulative distribution functions
    \(F_1(\cdot|\mathbf{\theta}_1), ..., F_d(\cdot|\mathbf{\theta}_d)\), one can write
    an expression for the density of \(\mathbf{X}\) as in \eqref{eq:density_from_sklar}:
    \[
    f_\mathbf{X}(\mathbf{x})=c_{\mathbf{X}}(F_{X_1}(x_1), ..., F_{X_d}(x_d))\prod_{i=1}^d f_{X_i}(x_i).
    \]
Given a vector of \(n\) observations \(\mathbf{D}_{1:n}=(\mathbf{D}_1, ..., \mathbf{D}_n)\)
    one can then attempt
    to maximise the log-likelihood
    <div style="overflow-x: auto;">
\begin{align*}
    \log L(\mathbf{\theta}_1, ..., \mathbf{\theta}_d, \mathbf{\theta}_C)
    &= \prod_{i=1}^n f_{\mathbf{X}}(\mathbf{D}_i) \newline
    &= \sum_{i=1}^n\Big(
    \sum_{j=1}^d\log f_\mathbf{X_j}(D_{i,j}|\mathbf{\theta}_j) \newline
    &\phantom{=\sum_{i=1}^n\Big(l}
    +\log c_{\mathbf{X}}\Big(F_{X_1}(D_{i,1}|\mathbf{\theta}_1),...,
    F_{X_d}(D_{i,d}|\mathbf{\theta}_d)\Big|\mathbf{\theta}_C\Big)

    \Big),
    \end{align*}
        </div>
    so as to obtain the ML estimators
    \(\hat{\mathbf{\theta}}_1, ..., \hat{\mathbf{\theta}}_d, \hat{\mathbf{\theta}}_C\).
    The problem with this approach is found in the very large number of parameters
    to be estimated, which complicates optimisation. Moreover, the mispecification of
    one single univariate distributions \(F_{X_i}(\cdot|\mathbf{\theta}_i)\) can introduce
    biases for all other marginals as well as for the copula.
    <br>
    One alternative is provided by pseudo-MLE whereby one splitts the estimation
    into a two-step procedure, estimating the marginals first,
    and the copula in a second stage.
    In particular, one starts by estimating the marginal cumulative distribution functions
    \(F_{X_i}(\cdot|\mathbf{\theta}_i)\) yielding the estimator \(\hat{F}_{X_i}\), either using a parametric model to obtain
    a univariate ML-estimator for \(\hat{\mathbf{\theta}}_i\), or even considering the
    empirical cumulative distribution function. Then, one estimates the copula as a second
    step, again maximising the log-likelihood
    <div style="overflow-x: auto;">
    \[
\prod_{i=1}^nf_\mathbf{X}(\mathbf{D}_i) = \sum_{i=1}^n \log\left(c_\mathbf{X}(\hat{F}_{X_1}(D_{i,1}), ..., \hat{F}_{X_d}(D_{i,d})|
    \mathbf{\theta}_C)\right).
    \]
        </div>
    <br>
    Yet another alternative is moment-matching.
    <br><br>
    </i></blockquote>
    <b>Proposition 14</b><blockquote><i>
For Gaussian and Student-t distributed \(\mathbf{X}=(X_1, ..., X_d)\),
    and Gaussian or Student-t copulae with covariance matrix \(\Omega\),
    the following holds:
    \begin{equation}
\rho_\tau(X_i, X_j) = \frac{2}{\pi}\arcsin(\Omega_{ij}),
    \end{equation}
    where \(\rho_\tau\) denotes Kendall's tau.
</i></blockquote>
<br>


A number of non-parametric methods exist as well, such as the
    empirical Bernstein copula <a href="#ref6">[6]</a>
    as well as kernel smoothing estimators.
The Python package
    <a href="https://openturns.github.io/openturns/latest/index.html"><i><b>OpenTURNS</b></i></a>
    provides
    <a href="https://openturns.github.io/openturns/latest/auto_data_analysis/estimate_dependency_and_copulas/plot_estimate_non_parametric_copula.html#sphx-glr-auto-data-analysis-estimate-dependency-and-copulas-plot-estimate-non-parametric-copula-py">
        <u>utilities</u>
    </a>
    to fit these non-parametric copula models.



    <h4>Estimation in practice: parametric estimates</h4>

    Practically, python has a number of packages (although still in their infancy)
    which implement similar methods.  Popular ones are
    <a href="https://sdv.dev/Copulas/index.html"><i><b>Copulas</b></i></a>,
    <a href="https://copulae.readthedocs.io/en/latest/index.html"><i><b>Copulae</b></i></a>
    and
    <a href="https://openturns.github.io/openturns/latest/index.html"><i><b>OpenTURNS</b></i></a>.

    Let's consider a few example with
    <a href="https://openturns.github.io/openturns/latest/index.html"><i><b>OpenTURNS</b></i></a>.

    The simplest case is that of estimating the gaussian copula associated with
    a multivariate normal distribution, which effectively boils down
    to estimating the correlation matrix.
    The code
    <pre><code class="lang-python">import openturns as ot, numpy as np, scipy.stats as st

np.random.seed(seed=0)
mvnorm = st.multivariate_normal(mean=[0, 0], cov=[[1., 0.75],
                                                  [0.75, 1.]])
X = mvnorm.rvs(10_000)

dist = ot.NormalCopulaFactory().build(X)
print(dist)</code></pre>
    returns
    <div style="overflow-x: auto;">
    <pre>
      NormalCopula(R = [[ 1        0.754492 ]
                        [ 0.754492 1        ]])</pre>
    </div>
The parameters can then be extracted as
    <pre><code class="lang-python">print(dist.getParameter())</code></pre>
    which returns
    <pre>
      [0.754492]</pre>
Similarly, for data with gaussian copula but non-gaussian marginals:
<pre><code class="lang-python">import openturns as ot, numpy as np, scipy.stats as st

np.random.seed(seed=0)
mvnorm = st.multivariate_normal(mean=[0, 0], cov=[[1., 0.75],
                                                  [0.75, 1.]])
X = mvnorm.rvs(10_000)
U = st.norm().cdf(X)
X_trans = np.array([st.beta(a=3, b=10).ppf(U[:,0]),
                    st.gumbel_l().ppf(U[:,1])]).T

dist = ot.NormalCopulaFactory().build(X_trans)
print(dist)</code></pre>
    one obtains
    <div style="overflow-x: auto;">
    <pre>
      NormalCopula(R = [[ 1        0.754492 ]
                        [ 0.754492 1        ]])</pre>
    </div>
Notice that having different non-gaussian marginals
    does not affect the estimates of the parameters.
    <figure>
        <img class="center" alt="copulae_grid"
             src="img/Copulae/Gcop_estimate_convergence.svg"
             width="70%" >
        <figcaption><i><b>Fig.9:</b> Convergence of the estimate
            of the linear correlation parameter \(\rho\) as
            the sample size \(N\) increases. The shaded area represents
         the one standard deviation error around the mean estimate. </i></figcaption>
    </figure>
Finally, consider the following example
    for the non-gaussian Clayton copula with \(\theta=2\)
    and \(\text{Beta}(\alpha=3, \beta=10)\) and
    \(\text{Gumbel}(\mu=0,\beta=1)\)
    marginals:
    the code
    <pre><code class="lang-python">import openturns as ot, numpy as np, scipy.stats as st

J = ot.ComposedDistribution([ot.Uniform(0, 1),
                             ot.Uniform(0, 1)],
                            ot.ClaytonCopula(2))
U1, U2 = np.array(J.getSample(10_000)).T

X_trans = np.array([st.beta(a=3, b=10).ppf(U1),
                    st.gumbel_l().ppf(U2)]).T

dist = ot.ClaytonCopulaFactory().build(X_trans)
print(dist)</code></pre>
    returns
    <pre>
      ClaytonCopula(theta = 1.98901)
    </pre>

    <h4>Estimation in practice: non-parametric estimates</h4>
As briefly mentioned
    <a href="https://openturns.github.io/openturns/latest/index.html"><i><b>OpenTURNS</b></i></a>
    also allows us to estimate copulae non-parametrically.
    Let us start by sampling from a bivariate normal distribution with
    linear correlation coefficient \(\rho=0.75\):
    <pre><code class="lang-python">import openturns as ot, numpy as np, scipy.stats as st

N = 1_000
mvnorm = st.multivariate_normal(mean=[0, 0], cov=[[1., 0.75],
                                                  [0.75, 1.]])
X = mvnorm.rvs(N)</code></pre>

Now, one can extract a
    <a href="https://openturns.github.io/openturns/latest/user_manual/_generated/openturns.EmpiricalBernsteinCopula.html">
        <i>Bernstein</i></a> non-parametric estimate simply as

    <pre><code class="lang-python">Bs_Est = ot.EmpiricalBernsteinCopula(X, 20, False)</code></pre>

where the second argument is the number of bins which each dimension of the unit cube
    \([0,1]^d\) is divided into in order to estimate the empirical copula.
    Similarly, one can obtain a
    <a href="https://openturns.github.io/openturns/latest/theory/data_analysis/kernel_smoothing.html"><i>Kernel Smoothing</i></a>
    estimation:

    <pre><code class="lang-python">KS_Est = ot.KernelSmoothing().build(X).getCopula()</code></pre>

It is then possible to obtain a new sample of <code>N</code> points
    from the estimated copulae
    <code>Bs_Est</code> and
    <code>KS_Est</code>
    as <code>Bs_Est.getSample(N)</code> and
    <code>KS_Est.getSample(N)</code> respectively.

    <figure>
        <img class="center" alt="copulae_grid"
             src="img/Copulae/NonParametric_estimates.svg"
             width="100%" >
        <figcaption><i><b>Fig.10:</b> Left: the gaussian copula density
            with \(\rho=0.75\).
            Center: Bernstein non-parametric copula density estimate for a \(N=1000\) sample
            from a multivariate normal distribution with \(\rho=0.75\).
            Right: Kernel Smoothing non-parametric copula density estimate for a \(N=1000\) sample
            from a multivariate normal distribution with \(\rho=0.75\).</i></figcaption>
    </figure>





    <br>

    <h4 id="REFERENCES"><i>References</i></h4>

    [1] <a href="https://press.princeton.edu/books/hardcover/9780691166278/quantitative-risk-management" id="ref1">"Quantitative Risk Management: Concepts, Techniques and Tools", Alexander J. McNeil, Paul Embrechts, and Rüdiger Frey, 2005</a>
    <br>
    [2] <a href="https://www.springer.com/gp/book/9780387286594">  "An Introduction to Copulas", Roger B. Nelsen, 2006</a>
    <br>
    [3] <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.321.5607&rep=rep1&type=pdf"> "Correlation and Dependence in Risk Management: Properties and Pitfalls", Paul Embrechts, Alexander McNeil, and Daniel Straumann, 1999</a>
    <!--    [2]<a href="https://www.routledge.com/Dependence-Modeling-with-Copulas/Joe/p/book/9781466583221"> "Dependence Modeling with Copulas", Harry Joe (2015) </a>-->
    <br>
    [4] <a href="https://www.amazon.co.uk/Intro-Probability-Vol-2e-Statistics/dp/0471257095/ref=sr_1_6?dchild=1&keywords=An+Introduction+to+Probability+Theory+and+Its+Applications%2C+Volume+2&qid=1628511222&s=books&sr=1-6" id="ref4"> "An Introduction to Probability Theory and Its Applications", W. Feller, 1971, volume 2, 2nd edition; John Wiley & Sons; p. 439</a>
    <br>
    [5] <a href="https://www.jstor.org/stable/2289314" id="ref5"> “Families of Multivariate Distributions.”,
    A.W. Marshall, I. Olkin, 1988,
    Journal of the American Statistical Association, 83, 834–841</a>
    <br>
    [6] <a href="https://www.jstor.org/stable/3533531" id="ref6"> “The Bernstein copula and its applications to modeling and approximations of multivariate distributions.”,
    Sancetta, A. and S. Satchell, 2004,
    Econometric Theory 20(3), 535–562</a>
    <br><br>

    <!-- ________________________________________________________________________________________________ -->
    <!-- ________________________________________________________________________________________________ -->
    <!-- ________________________________________________________________________________________________ -->
<!--    <h7>-->
        <!-- <span style="float:right;">
       <a href="./L2.html"><b><img src="../Icons/next.png" width="20px" align="right">Next</b></a>
       </span>
       </h7>
       <br> -->

        <h5> <a href="../Teaching.html"><b><img src="../Icons/back.png" width="20px"> Back to Teaching</b></a></h5>



        <hr class="w3-opacity">
</div>
</div>
<script type="text/javascript" src="../js/footer.js"></script>

<!-- End page content -->
</div>

<script src="../js/OpenCloseSidebar.js"></script>
<!--<script src="../js/ModalImageGallery.js"></script>-->
<script src="../js/OpenCloseCollapsible.js"></script>

</body>
</html>
