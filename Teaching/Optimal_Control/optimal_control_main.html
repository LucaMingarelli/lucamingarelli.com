<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Oribi Analytics -->
  <script type="application/javascript">
    (function(b,o,n,g,s,r,c){if(b[s])return;b[s]={};b[s].scriptToken="XzgzMjUyODI5Ng";b[s].callsQueue=[];b[s].api=function(){b[s].callsQueue.push(arguments);};r=o.createElement(n);c=o.getElementsByTagName(n)[0];r.async=1;r.src=g;r.id=s+n;c.parentNode.insertBefore(r,c);})(window,document,"script","https://cdn.oribi.io/XzgzMjUyODI5Ng/oribi.js","ORIBI");
  </script>
  <!-- End Oribi Analytics -->
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
          j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
          'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-NLJ2P23');</script>
  <!-- End Google Tag Manager -->
  <meta charset="utf-8"/>
  <title>Luca Mingarelli</title>
  <link rel="icon" href="/Icons/stork.png" type="image/png">
  <style>
    .collapsible {background-color: #777;color: white;cursor: pointer;  padding: 2px;width: 100%;
      border: none;  text-align: left;  outline: none;  font-size: 15px; border-radius: 2px;}
    /*.active,*/
    .collapsible:hover {  background-color: #555;}
    .content {display: none;  overflow: hidden;  background-color: #DCDCDC;}
  </style>

</head>

<!--<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>-->
<script>window.MathJax = {tex: {tags: 'ams'}};</script>
<script id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<!--<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG"></script>-->

<link rel="stylesheet" href="/PRISM/prism.css"> <!-- For code highlight -->
<script src="/PRISM/prism.js"></script>         <!-- For code highlight -->


<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="/css/w3.css">
<link rel="stylesheet" href="/css/font.css">
<link rel="stylesheet" href="/css/responsiveiframe.css">
<script src="/js/style_preamble.js"></script>

<body class="w3-light-grey w3-content" style="max-width:2600px">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NLJ2P23"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- NavigationBar -->
<script src="/js/Navbar.js"></script>
<!-- _____________ -->
<!-- PRE_CONTENT -->
<script src="/js/pre_content.js"></script>
<!-- _____________ -->


<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- Load d3.js -->
<script src="https://d3js.org/d3.v5.js"></script>



<div class="w3-content w3-justify" style="max-width:800px">

  <h1 style="text-align: left;"><b>Optimal Control Theory</b></h1>

<!--  <h2 style="text-align: left;"><b>The <i>efficiency-generality</i> trade-off</b></h2>-->

<h3 style="text-align: left;"><b>Defining the Problem</b></h3>
  

  <b>Definition 1 — Control function</b><blockquote><i> 
    A <strong>control function</strong>
    \(\mathbf{\gamma}(t)\) is a map from time \(t\in [0, \infty)\) to the set of <i>admissable controls</i> \(\Gamma\).
    <br>
    The control is also called a <strong>policy</strong>.
    
    </blockquote></i>
    <br>

    <b>Definition 2 — Controlled Dynamical System</b><blockquote><i> A system whose dynamics is described by the equation of motion
      
      \begin{equation}
      \dot{\mathbf{x}}(t)=\mathbf{f}(\mathbf{x}(t), \mathbf{\gamma}(t), t),
      \end{equation}
      
      is called a <strong>\(\mathbf{\gamma}\)-Controlled Dynamical System</strong>. 
      <br>
      A system which does not explicitely depent on time 
  
      \begin{equation}
      \dot{\mathbf{x}}(t)=\mathbf{f}(\mathbf{x}(t), \mathbf{\gamma}(t)),
      \end{equation}

      is called <strong>Autonomous</strong>.
      <br>
      The <strong>state</strong> of the system \(\mathbf{x}(t)\in \mathbb{R}^n\) is also called <strong>trajectory</strong>.
  

      </blockquote></i>
      <br>

      <b>Definition 3 — Payoff Functional</b><blockquote><i> A functional of the form
        <div style="overflow-x: auto; overflow-y: hidden;">
        \begin{equation}
        \mathbf{P}[\mathbf{\gamma}] := \int_0^T L(\mathbf{x}(t), \mathbf{\gamma}(t)) dt + \phi(\mathbf{x}(T)),
        \end{equation}
        </div>
        is called a <strong>Payoff Functional</strong>, 
        where the function \(L\) is called <strong>running payoff</strong> or <strong>Lagrangian</strong> 
        and \(\phi\) is called <strong>terminal payoff</strong> or <strong>boundary cost</strong> (sometimes also referred to as Mayer's Term).
        </blockquote></i>
        <br>
  
  

  <b>Definition 4 — Optimal Control Problem</b><blockquote><i> 
    Given a system with controls \(\mathbf{\gamma}\) 
    and payoff functional \(\mathbf{P}[\mathbf{\gamma}]\), 
    the associated <strong>Optimal Control Problem </strong> amounts to finding 
    an <strong>optimal control</strong> \(\mathbf{\gamma}^*\) maximising the payoff: 
    
    \begin{equation}
    \mathbf{P}[\mathbf{\gamma}^*] = \max_{\mathbf{\gamma}\in\Gamma} \mathbf{P}[\mathbf{\gamma}],
    \end{equation}

    that is
    \begin{equation}
    \mathbf{P}[\mathbf{\gamma}^*] \ge \mathbf{P}[\mathbf{\gamma}], \quad \forall \mathbf{\gamma}\in\Gamma.
    \end{equation}

    <br>
    The problem is called a Lagrange problem when \(\phi=0\), and a Mayer problem when \(L=0\).
    When both \(L\ne0\) and \(\phi\ne0\) it is also called a Bolza Problem <a href="#ref1">[1]</a>.

    </blockquote></i>
    
    
<hr class="w3-opacity">

<h6>Exercise 1</h6>
Show that a problem in the Lagrange form can be turned into a Mayer problem with some appropriate transformation.
<br>
<button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 1</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
    Consider a Lagrange problem with running cost \(L(x, \gamma, t)\). Introduce an extra state variable \(\tilde{x}\) such that
    \begin{equation}
    \begin{split}
    \dot{\tilde{x}}_0 &= L(x, \gamma, t),\\
    \tilde{x}(t=0) &= 0.
    \end{split}
    \end{equation}
    Clearly this is such that 
    \begin{equation}
    \tilde{x}(t=T) = \int_0^TL(x(t), \gamma(t), t) dt . 
    \end{equation}
    Therefore, the problem expressed in terms of \(\tilde{x}\) is a Mayer problem.


    <p align="right"> ◻</p>
</div>

<h6>Exercise 2</h6>
Show that a problem in the Mayer form can be turned into a Lagrange problem with some appropriate transformation.
<br>
<button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 2</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
    Consider a Mayer problem with terminal cost \(\phi(x, \gamma, t)\). Notice that
    \begin{equation}
    \begin{split}
    \phi(x(T), T) &= \phi(x(0), 0) + \int_0^T\frac{d}{dt}\phi(x(t), t) dt \\
                  &= \phi(x(0), 0) + \int_0^T\left(\partial_t \phi(x(t), t)\right) + \left(\partial_x\phi(x(t), t)\cdot f(x(t), \gamma(t), t)\right) dt.
    \end{split}
    \end{equation}
    Therefore, the problem expressed in these terms is a Lagrange problem.


    <p align="right"> ◻</p>
</div>

<hr class="w3-opacity">

    <br>

    Real world applications are abundant, from minimising fuel consumption when designing space missions, to optimising the production of chemicals, 
    from maximisation of throughput of information transmition over a communication channel, to the determination of optimal policy paths for monetary policy setting. 
    Even more so, optimality can be considered to a large extent a universal principle of nature, in the sense that
    most processess seem to be optimising over some specific quantity. 
    This is true for nature's fundamentals, where often one can describe physical systems via principles of least action, 
    for biological systems, e.g. explaining animals foraging behaviour, up to societies whose individuals maximise their respective expected utilities.
    Furthermore, optimisaion clearly also plays a crucial role in many 



    <h3 style="text-align: left;"><b>The Maximum Principle</b></h3>


    <b>Definition 5 — Hamiltonian</b><blockquote><i> 
      Given a system with controls \(\mathbf{\gamma}\), 
      payoff functional \(\mathbf{P}[\mathbf{\gamma}]\),
      and dynamics of the state variable \(\mathbf{x}\) 
      governed by the law of motion 
      \(\dot{\mathbf{x}}(t)=\mathbf{f}(\mathbf{x}(t), \mathbf{\gamma}(t), t)\)
      
      the <strong>control Hamiltonian</strong> is
      <div style="overflow-x: auto; overflow-y: hidden;">
      \begin{equation}
      H(\mathbf{x}(t), \mathbf{\gamma}(t), \lambda(t), t) := L(\mathbf{x}(t), \mathbf{\gamma}(t), t) +\lambda^T(t)  \mathbf{f}(\mathbf{x}(t), \mathbf{\gamma}(t), t),
      \end{equation}     
      </div>
      where \(L\) is the Lagrangian term in \(\mathbf{P}\) (running payoff),
      and \(\lambda\) is called the <strong>co-state</strong> variable.
      
    
    </blockquote></i>
    <br>
  Notice that the multiplier (or co-state) \(\lambda(t)\), 
  unlike the Lagrangian multiplier in static optimisation, depends on time </strong><a href="#ref3">[3]</a>.
  <br>



  <br><br>

  <b>Theorem 6 — Pontryagin Maximum Principle</b><blockquote><i> 
    Assume the optimal control \(\mathbf{\gamma}^*\) is known and admissable (\(\mathbf{\gamma}^*\in\Gamma\)), 
    which generates the associated optimal trajectory \(\mathbf{x}^*\). 
    Then, there exists and adjoint trajectory \(\mathbf{p}^*\) conjugate to \(\mathbf{x}^*\) 
    such that
    <div style="overflow-x: auto; overflow-y: hidden;">
    \begin{equation}
    \begin{split}
    \dot{\mathbf{x}}(t) &= \nabla_\lambda H(\mathbf{x}^*, \mathbf{\gamma}^*, \lambda^*, t) \quad\quad\quad\quad \phantom{-} (\text{state equation})\\
    \dot{\lambda}(t) &= -\nabla_\mathbf{x} H(\mathbf{x}^*, \mathbf{\gamma}^*, \lambda^*, t) \quad\quad\quad\quad (\text{co-state equation})\\
     H(\mathbf{x}^*, \mathbf{\gamma}^*, \lambda^*, t) &\ge H(\mathbf{x}^*, \mathbf{\gamma}^*, \lambda, t), \quad \forall \gamma \in \Gamma \quad \phantom{-.} (\text{Pontryagin Maximum Principle})
    \end{split}
    \end{equation}     
    </div>

  </blockquote></i>
  <br>

  In other words, Pontryagin's Maximum Principle provides a necessary condition for optimality.
  
  <br><br>
  
  <b>Theorem xxx — Transversality Conditions</b><blockquote><i> 
    xxx

  </blockquote></i>
  <br>


  <br><br>




<b>Example 1 — Minimal time for bringing a particle to rest at origin </b>
<br><br>
<i>
Consider the one-dimensional problem of bringing a particle to rest at \(x=0\) in minimal time.
The initial position and velocity of the particle are \(x_0\) and \(v_0\) respectively.

The control is the force \(f(t)\) applied to the particle, such that \(|f(t)|\le 1\).
<br>
Therefore, the problem is to minimise the total time 
\begin{equation}
\mathcal{T} = \int_0^T 1 dt,
\end{equation}
where \(T\) is the time when the particle reaches the origin, subject to the law of motion which,
recalling that \(v=\dot{x}\) and \(f=\dot{v}\), reads
\begin{equation}
 \frac{d}{dt} \begin{bmatrix} x \\ v  \end{bmatrix} = 
\left(\begin{matrix}
     0 & 1 \\
     0 & 0  
   \end{matrix}\right) \begin{bmatrix} x \\ v \end{bmatrix} 
   + \begin{bmatrix} 0 \\ 1 \end{bmatrix} f.
\end{equation}
<br>

To start off, one finds the Hamiltonian to be
<div style="overflow-x: auto; overflow-y: hidden;">
\begin{equation}
H = \mathbf{\lambda}^T\left(\begin{matrix}
0 & 1 \\
0 & 0  
\end{matrix}\right) \begin{bmatrix} x \\ v \end{bmatrix} 
+ \mathbf{\lambda}^T \begin{bmatrix} 0 \\ 1 \end{bmatrix} f - 1
= \lambda_1 v + \lambda_2 f - 1.
\label{double_integrator_hamiltonian}
\end{equation}
</div>

Because the term \(\lambda_2 f\) in the Hamiltonian is linear in \(f\), 
the maximum with respect to the control is achieved at one of the endpoints of the interval \([-1, 1]\), depending on the sign of \(\lambda_2\).
That is, the Hamiltonian is maximised by
<div style="overflow-x: auto; overflow-y: hidden;">
\begin{equation}
f^*=\begin{cases}
\text{sgn}\left(\lambda_2\right) & \text{if $\lambda_2 \ne 0$}\\
\text{any value} \in {-1,1} & \text{otherwise}
\end{cases} \quad.
\end{equation}
</div>
<br>

The costate variables are given by

\begin{equation}
\begin{split}
\dot{\lambda}_1 &= \partial_x H = 0, \\ 
\dot{\lambda}_2 &= \partial_v H = -\lambda_1.
\end{split}
\end{equation}

Denotining the values of the costate variables at termination by \(\bar{\lambda}_i = \lambda_i(t=T)\), 
we can therefore solve for the costate variables:

\begin{equation}
\begin{split}
\lambda_1 &= \bar{\lambda}_1, \\
\lambda_2 &= \bar{\lambda}_1 t + \bar{\lambda}_2.
\end{split}
\end{equation}

Here we observe that on the optimal trajectory, \(\lambda_2\) will switch sign at most once 
before termination at \(t_s=-\bar{\lambda}_2/\bar{\lambda}_1\), 
if \(\text{sgn}\left(\bar{\lambda}_1\right)\ne\text{sgn}\left(\bar{\lambda}_2\right)\). 
This means that the optimal control strategy consists of switching between maximal accelaration in one direction
to maximal acceleration in the opposite direction. This is therefore a bang-bang control.

<br>
Taking this result back to the equations of motion we find

<!-- dx = v -->
\begin{equation}
\begin{split}
\dot{v} &= f^* = \text{sgn}(\bar{\lambda}_1 t+ \bar{\lambda}_2) \\
\dot{x} &= v
\end{split}\quad,
\end{equation}

thus

\begin{equation}
\begin{split}
v &= \text{sgn}(\bar{\lambda}_1 t+ \bar{\lambda}_2) t + v_0 \\
x &= \frac{t^2}{2}\text{sgn}(\bar{\lambda}_1 t+ \bar{\lambda}_2) + v_0t + x_0 
\end{split}\quad.
\end{equation}

<br><br>

The terminal time \(T\) is free, therefore the transversality condition tells us \(H(t=T)=0\).
Hence, we can conclude that \(|\bar{\lambda}_2| = 1\).
<br>This allows one to find explicit solutions for \(x\) and \(v\). 
However, notice that one can also get rid of the parametrisation in \(t\) to find the associated orbits in the \(x\)-\(v\) phase plane. 
For \(f=\pm 1\)


  \begin{equation}
  \begin{split}
  x   &= \pm \frac{t^2}{2} + v_0t + x_0,\\
  v   &= \pm t + v_0,
  \end{split}
  \end{equation}

  or

  \begin{equation}
  x   = \pm \frac{v^2}{2} +(x_0\mp\frac{v_0}{2}).
  \end{equation}

  This family of parabulae intersects the origin only for \(v_0=\pm\sqrt{x_0}\) 
  which determines the switching curve \(x = -\frac{1}{2}|v|v\) (blue curve in Figure 1). Therefore, the optimal strategy is to apply \(f=1\) (\(f=-1\)) 
  if the initial condition lies below (above) the switching curve, follow the orbit up to the switching curve, 
  then switching the control value to  \(f=-1\) (\(f=1\)) and following the switching curve to the origin.


</i>

<figure style="text-align: center;justify-content: center;align-items: center;">
  <!-- Create a div where the graph will take place -->
  <div id="double_integrator_figure" style="width: 100%;"></div>
  <figcaption><i><b>Fig.1:</b> Optimal trajectory of the system described by equation \eqref{double_integrator_hamiltonian}, for different initial conditions (move your mouse to adjust them).
  </i></figcaption>
</figure>

<script src="./double_integrator.js"></script>

    The interested reader may find more details on this problem, also referred to as <i>double-integrator problem</i>, in <a href="#ref2">[2]</a>.
    
    <h3 style="text-align: left;"><b>Dynamic Programming</b></h3>

    A similar approach to the maximum principle developed during the cold war in the Soviet Union, 
    was developed in the same period in the United States. 
    However, while Pontryagin's maximum principle only yielded necessary conditions for optimality, 
    the <i>dynamic programming</i> approach pioneered by Bellman at RAND would yield both necessary and sufficient conditions for optimality.

    <br><br>
    Dynamic programming is a general approach: let us start by considering
    a discrete deterministic example. 
    We shall then move on to consider the continuous and stochastic extensions.

    <br>

    Consider a discrete system of the form 
    \begin{equation}
    x_{k+1} = f(x_k, \gamma_k), \quad k = 0, ..., T-1;
    \end{equation}
    here \(x_k\in X\subset\mathbb{N}\) and \(\gamma_k\in \Gamma \subset \mathbb{N}\).
     Furthermore, to each discrete step forward in time \(t\) is associated 
     the running payoff \(\ell(x_t, x_{t+1})\). Starting from \(x_0=0\), the objetive is to maximise 
     the total payoff.
    <br>
    The naive approach to solve this problem consists of enumerating each possible trajectory 
    starting from the initial state \(x_0\) up to time \(T\). 
    It's straightforward to find this approach implies evaluating \(|X|^T\) trajectories. 
    The evaluation of the payoff for each trajectory requires \(T\) operations, 
    so we denote by \(\mathcal{O}(|X|^TT)\) the computational complexity of this approach.
    <br>
    This is brute force search is depicted in Fig 2 left, where paths explored multiple times appear darker.
    <br>
    However, a better approach is possible. Let us first introduce an observation which will be crucial in the following.
    <br><br>
    <b>Definition 8 — Bellman's Principle of Optimality</b><blockquote><i> 

      For any point \(\tau>0\) on an optimal trajectory \(\mathbf{x}(t)\), 
      the remaining trajectory \(\mathbf{x}(t>\tau)\) is optimal 
      for the corresponding problem initiated at \(\tau\).
    
    </blockquote></i>
    <br>
    Or, in Bellman's own words <a href="#ref3">[3]</a>: 
    "An optimal policy has the property that whatever 
    the initial state and initial decision are, 
    the remaining decisions must constitute an optimal policy 
    with regard to the state resulting from the first decision."
    <br>
    Bellman's optimality principle therefore guarantees that from any point \(x_t\) on the optimal path,
    one can discard paths going backwards in time being certain that they are not portions of optimal trajectories. 
    <br>
    In order to leverage this intuition, let us thus define 
    at each time \(t\) the value function
     \begin{equation*}
     V(x_t) = \max\left(\sum_{s=t+1}^T \ell(x_s, x_s+1)\right).
     \end{equation*}
    Clearly this represents the largest payoff attainable starting from \(x_t\). 
    The introduction of \(V(x_t)\) is useful as it allows us to start at \(x_T\) and proceed backwards in time.
    At \(t=T\) terminal costs are known for all possible choices of \(x_T \in X\). 
    At \(t=T-1\), for each possible value \(x_{T-1} \in X\) we can compute the optimal payoff 
    as the sum of the one step running cost plus the terminal payoff, and record that.
    Then, one can repeat this procedure for each \( t < T - 1 \) up to \(t=0\). 
    Should one step involve two equivalent paths, either can be chosen at random.
    This process is depicted in Fig2, right.

     <figure style="text-align: center;justify-content: center;align-items: center;">
      <!-- Create a div where the graph will take place -->
      <div id="dp0_figure" style="width: 100%;"></div>
      <figcaption><i><b>Fig.2:</b> Brute force search (left) versus dynmamics programming approach (right). 
        Darker paths are traversed more often.
        Here \(|X| = 3\) and \(T=5\).
      </i></figcaption>
    </figure>
    
    <script src="./dp0.js"></script>
    
    <br>
    At this point, for every \(x_t\) we have an associated value function, which imply two major achievements. 
    First, \(V(x_0)\)  represents the optimal payoff.
    Second, starting from \(x_0\) and proceeding greedily with respect to the value function we are therefore guaranteed
    to find the optimal path.
    <br>
    With this dynamic programming<a href="#ref4">[4]</a> approach 

    






    <br><br>
    <b>Definition x — Bellman's Principle of Optimality</b><blockquote><i> 

      For any point \(\tau>0\) on an optimal trajectory \(\mathbf{x}(t)\), 
      the remaining trajectory \(\mathbf{x}(t>\tau)\) is optimal 
      for the corresponding problem initiated at \(\tau\).
    
    </blockquote></i>

    Or, in Bellman's own words <a href="#ref3">[3]</a>: 
    <br>
    "An optimal policy has the property that whatever 
    the initial state and initial decision are, 
    the remaining decisions must constitute an optimal policy 
    with regard to the state resulting from the first decision."

    <br><br>

    This notion can be formalised with the following theorem.
    <br><br>

    <b>Theorem x — Dynamic Programming</b><blockquote><i> 

      Given a Bolza problem over the time interval \([t_0, T]\), with payoff functional \(P[\gamma]\) 
      and subject to the law of motion \(\dot{x}(t)=f(x(t), \gamma(t), t)\),
      define the <b>Value Function</b>
      \begin{equation}
      V(x, t) := \inf_{\gamma_{[t_0, T]}}  P[\gamma].
      \end{equation}
      Then, one has
      \begin{equation}
      V(x, t) = \inf_{\gamma_{[t_0, T]}}  \left\{\int_t^{\tau}L(x(t), \gamma(t), t) dt + V(x(\tau), \tau)\right\}.
      \end{equation}

      for any \(\tau\ge t\), and where \(x\) on the right hand side is he state trajectory corresponding to the control \(\gamma_{[t_0, T]}\).
    </blockquote></i>

    <br><br>



    <h4 style="text-align: left;"><b> The Hamilton-Jacobi-Bellman equation</b></h4>


    
    <b>Theorem x — Hamilton-Jacobi-Bellman equation</b><blockquote><i> 

      For 
    </blockquote></i>

    <br><br>

    <h4 style="text-align: left;"><b> Stochastic DP and HJB equation</b></h4>


    <b>Theorem x — Stochastic Dynamic Programming</b><blockquote><i> 

      For 
    </blockquote></i>

    <br><br>


    <b>Theorem x — Stochastic HJB equation</b><blockquote><i> 

      For 
    </blockquote></i>

    <br><br>



    <br>






<br><br>
  <h4 id="REFERENCES"><i>References and notes</i></h4>

  [1] <a href="https://link.springer.com/article/10.1007/BF02419594" id="ref1">"Über zwei Euler’sche Aufgaben aus der Variationsrechnung", Oskar Bolza, 1913, Annali di Matematica Pura ed Applicata</a>
  <br>
  [2] <a href="https://arxiv.org/pdf/1909.03192.pdf" id="ref2">"Analytic Solution of the Time-Optimal Control of a Double Integrator from an Arbitrary State to the State-space Origin", Marcello Romano, and Fabio Curti, 2019. </a>
  <br>
  [3] <a href="https://gwern.net/doc/statistics/decision/1957-bellman-dynamicprogramming.pdf" id="ref3">"Dynamic Programming", Richard Bellman, 1957, Chap. III.3. </a>
  <br>
  [4] <normal id="ref4">Notice that while related, the Control Hamiltonian of optimal control theory is distinct from the Hamiltonian used in mechanics. 
    In particular, the latter is used to derive the equation of motion of a dynamical system. 
    The Hamiltonian of optimal control theory instead, is a concept developed by Lev Pontryagin, 
    with the purpose to provide conditions for extrimising a functional with respect to a control variable. <normal>
  <br>
  [5] <normal id="ref5">Why is this approach named <i>Dynamic Programming</i>?
    The term was coined in the early 50s by Richard Bellman while working on his research on multistage decision processes 
    at RAND Corporation, at that time fully funded by US government. 
    The justification for the label has a rather interesting story. 
    From Bellman's autobiography <i>Eye of the Hurricane: An Autobiography</i> (1984, page 159): 
    <i>I spent the Fall quarter (of 1950) at RAND. 
      My first task was to find a name for multistage decision processes. 
      An interesting question is, 
      "Where did the name, dynamic programming, come from?" 
      The 1950s were not good years for mathematical research. 
      We had a very interesting gentleman in Washington named Wilson.
      He was Secretary of Defense, and he actually had a pathological fear and hatred of the word "research". 
      I'm not using the term lightly; I'm using it precisely. 
      His face would suffuse, he would turn red, 
      and he would get violent if people used the term research in his presence. 
      You can imagine how he felt, then, about the term mathematical. 
      The RAND Corporation was employed by the Air Force, and the Air Force had Wilson as its boss, 
      essentially. Hence, I felt I had to do something to shield Wilson and the Air Force 
      from the fact that I was really doing mathematics inside the RAND Corporation. 
      What title, what name, could I choose? 
      In the first place I was interested in planning, in decision making, in thinking. 
      But planning, is not a good word for various reasons. 
      I decided therefore to use the word "programming". 
      I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying. 
      I thought, let's kill two birds with one stone. 
      Let's take a word that has an absolutely precise meaning, namely dynamic, 
      in the classical physical sense. 
      It also has a very interesting property as an adjective, 
      and that is it's impossible to use the word dynamic in a pejorative sense. 
      Try thinking of some combination that will possibly give it a pejorative meaning. 
      It's impossible. Thus, I thought dynamic programming was a good name. 
      It was something not even a Congressman could object to. 
      So I used it as an umbrella for my activities.</i>
    <normal>

<!--  https://bookdown.org/egarpor/NP-UC3M/kde-ii-asymp.html-->
<!--  https://bookdown.org/egarpor/NP-EAFIT/intro-nonpar.html-->

<!--  https://bookdown.org/egarpor/inference/-->
<!--  https://bookdown.org/egarpor/PM-UC3M/-->


  <br><br>

  <!-- ________________________________________________________________________________________________ -->
  <!-- ________________________________________________________________________________________________ -->
  <!-- ________________________________________________________________________________________________ -->
  <!--    <h7>-->
  <!-- <span style="float:right;">
 <a href="./L2.html"><b><img src="../Icons/next.png" width="20px" align="right">Next</b></a>
 </span>
 </h7>
 <br> -->

  <h5> <a href="/Teaching.html"><b><img src="/Icons/back.png" width="20px"> Back to Teaching</b></a></h5>



  <hr class="w3-opacity">
</div>
</div>
<script type="text/javascript" src="/js/footer.js"></script>

<!-- End page content -->
</div>

<script src="/js/OpenCloseSidebar.js"></script>
<!--<script src="../js/ModalImageGallery.js"></script>-->
<script src="/js/OpenCloseCollapsible.js"></script>

</body>
</html>
