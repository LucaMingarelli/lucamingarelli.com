<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Oribi Analytics -->
  <script type="application/javascript">
    (function(b,o,n,g,s,r,c){if(b[s])return;b[s]={};b[s].scriptToken="XzgzMjUyODI5Ng";b[s].callsQueue=[];b[s].api=function(){b[s].callsQueue.push(arguments);};r=o.createElement(n);c=o.getElementsByTagName(n)[0];r.async=1;r.src=g;r.id=s+n;c.parentNode.insertBefore(r,c);})(window,document,"script","https://cdn.oribi.io/XzgzMjUyODI5Ng/oribi.js","ORIBI");
  </script>
  <!-- End Oribi Analytics -->
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
            new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
          j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
          'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-NLJ2P23');</script>
  <!-- End Google Tag Manager -->
  <meta charset="utf-8"/>
  <title>Luca Mingarelli</title>
  <link rel="icon" href="/Icons/stork.png" type="image/png">
  <style>
    .collapsible {background-color: #777;color: white;cursor: pointer;  padding: 2px;width: 100%;
      border: none;  text-align: left;  outline: none;  font-size: 15px; border-radius: 2px;}
    /*.active,*/
    .collapsible:hover {  background-color: #555;}
    .content {display: none;  overflow: hidden;  background-color: #DCDCDC;}
  </style>

</head>

<!--<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>-->
<script>window.MathJax = {tex: {tags: 'ams'}};</script>
<script id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<!--<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG"></script>-->

<link rel="stylesheet" href="/PRISM/prism.css"> <!-- For code highlight -->
<script src="/PRISM/prism.js"></script>         <!-- For code highlight -->


<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="/css/w3.css">
<link rel="stylesheet" href="/css/font.css">
<link rel="stylesheet" href="/css/responsiveiframe.css">
<script src="/js/style_preamble.js"></script>

<body class="w3-light-grey w3-content" style="max-width:2600px">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NLJ2P23"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- NavigationBar -->
<script src="/js/Navbar.js"></script>
<!-- _____________ -->
<!-- PRE_CONTENT -->
<script src="/js/pre_content.js"></script>
<!-- _____________ -->


<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- Load d3.js -->
<script src="https://d3js.org/d3.v5.js"></script>



<div class="w3-content w3-justify" style="max-width:800px">

  <h1 style="text-align: left;"><b>Optimal Control Theory</b></h1>

<!--  <h2 style="text-align: left;"><b>The <i>efficiency-generality</i> trade-off</b></h2>-->

<h3 style="text-align: left;"><b>Defining the Problem</b></h3>
  

  <b>Definition 1 — Control function</b><blockquote><i> 
    A <strong>control function</strong>
    \(\mathbf{\gamma}(t)\) is a map from time \(t\in [0, \infty)\) to the set of <i>admissable controls</i> \(\Gamma\).
    <br>
    The control is also called a <strong>policy</strong>.
    
    </blockquote></i>
    <br>

    <b>Definition 2 — Controlled Dynamical System</b><blockquote><i> A system whose dynamics is described by the equation of motion
      
      \begin{equation}
      \dot{\mathbf{x}}(t)=\mathbf{f}(\mathbf{x}(t), \mathbf{\gamma}(t), t),
      \end{equation}
      
      is called a <strong>\(\mathbf{\gamma}\)-Controlled Dynamical System</strong>. 
      <br>
      A system which does not explicitely depent on time 
  
      \begin{equation}
      \dot{\mathbf{x}}(t)=\mathbf{f}(\mathbf{x}(t), \mathbf{\gamma}(t)),
      \end{equation}

      is called <strong>Autonomous</strong>.
      <br>
      The <strong>state</strong> of the system \(\mathbf{x}(t)\in \mathbb{R}^n\) is also called <strong>trajectory</strong>.
  

      </blockquote></i>
      <br>

      <b>Definition 3 — Payoff Functional</b><blockquote><i> A functional of the form
        <div style="overflow-x: auto; overflow-y: hidden;">
        \begin{equation}
        \mathbf{P}[\mathbf{\gamma}] := \int_0^T L(\mathbf{x}(t), \mathbf{\gamma}(t)) dt + \phi(\mathbf{x}(T)),
        \end{equation}
        </div>
        is called a <strong>Payoff Functional</strong>, 
        where the function \(L\) is called <strong>running payoff</strong> or <strong>Lagrangian</strong> 
        and \(\phi\) is called <strong>terminal payoff</strong> or <strong>boundary cost</strong> (sometimes also referred to as Mayer's Term).
        </blockquote></i>
        <br>
  
  

  <b>Definition 4 — Optimal Control Problem</b><blockquote><i> 
    Given a system with controls \(\mathbf{\gamma}\) 
    and payoff functional \(\mathbf{P}[\mathbf{\gamma}]\), 
    the associated <strong>Optimal Control Problem </strong> amounts to finding 
    an <strong>optimal control</strong> \(\mathbf{\gamma}^*\) maximising the payoff: 
    
    \begin{equation}
    \mathbf{P}[\mathbf{\gamma}^*] = \sup_{\mathbf{\gamma}\in\Gamma} \mathbf{P}[\mathbf{\gamma}],
    \end{equation}

    that is
    \begin{equation}
    \mathbf{P}[\mathbf{\gamma}^*] \ge \mathbf{P}[\mathbf{\gamma}], \quad \forall \mathbf{\gamma}\in\Gamma.
    \end{equation}

    <br>
    The problem is called a Lagrange problem when \(\phi=0\), and a Mayer problem when \(L=0\).
    When both \(L\ne0\) and \(\phi\ne0\) it is also called a Bolza Problem <a href="#ref1">[1]</a>.

    </blockquote></i>
    
    
<hr class="w3-opacity">

<h6>Exercise 1</h6>
Show that a problem in the Lagrange form can be turned into a Mayer problem with some appropriate transformation.
<br>
<button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 1</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
    Consider a Lagrange problem with running cost \(L(x, \gamma, t)\). Introduce an extra state variable \(\tilde{x}\) such that
    \begin{equation}
    \begin{split}
    \dot{\tilde{x}}_0 &= L(x, \gamma, t),\\
    \tilde{x}(t=0) &= 0.
    \end{split}
    \end{equation}
    Clearly this is such that 
    \begin{equation}
    \tilde{x}(t=T) = \int_0^TL(x(t), \gamma(t), t) dt . 
    \end{equation}
    Therefore, the problem expressed in terms of \(\tilde{x}\) is a Mayer problem.


    <p align="right"> ◻</p>
</div>

<h6>Exercise 2</h6>
Show that a problem in the Mayer form can be turned into a Lagrange problem with some appropriate transformation.
<br>
<button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 2</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
    Consider a Mayer problem with terminal cost \(\phi(x, \gamma, t)\). Notice that
    \begin{equation}
    \begin{split}
    \phi(x(T), T) &= \phi(x(0), 0) + \int_0^T\frac{d}{dt}\phi(x(t), t) dt \\
                  &= \phi(x(0), 0) + \int_0^T\left(\partial_t \phi(x(t), t)\right) + \left(\partial_x\phi(x(t), t)\cdot f(x(t), \gamma(t), t)\right) dt.
    \end{split}
    \end{equation}
    Therefore, the problem expressed in these terms is a Lagrange problem.


    <p align="right"> ◻</p>
</div>

<hr class="w3-opacity">

    <br>

    Real world applications are abundant, from minimising fuel consumption when designing space missions, to optimising the production of chemicals, 
    from maximisation of throughput of information transmition over a communication channel, to the determination of optimal policy paths for monetary policy setting. 
    Even more so, optimality can be considered to a large extent a universal principle of nature, in the sense that
    most processess seem to be optimising over some specific quantity. 
    This is true for nature's fundamentals, where often one can describe physical systems via principles of least action, 
    for biological systems, e.g. explaining animals foraging behaviour, up to societies whose individuals maximise their respective expected utilities.
    Furthermore, optimisaion clearly also plays a crucial role in many 



    <h3 style="text-align: left;"><b>The Maximum Principle</b></h3>


    <b>Definition 5 — Hamiltonian</b><blockquote><i> 
      Given a system with controls \(\mathbf{\gamma}\), 
      payoff functional \(\mathbf{P}[\mathbf{\gamma}]\),
      and dynamics of the state variable \(\mathbf{x}\) 
      governed by the law of motion 
      \(\dot{\mathbf{x}}(t)=\mathbf{f}(\mathbf{x}(t), \mathbf{\gamma}(t), t)\)
      
      the <strong>control Hamiltonian</strong> is
      <div style="overflow-x: auto; overflow-y: hidden;">
      \begin{equation}
      H(\mathbf{x}(t), \mathbf{\gamma}(t), \lambda(t), t) := L(\mathbf{x}(t), \mathbf{\gamma}(t), t) +\lambda^T(t)  \mathbf{f}(\mathbf{x}(t), \mathbf{\gamma}(t), t),
      \end{equation}     
      </div>
      where \(L\) is the Lagrangian term in \(\mathbf{P}\) (running payoff),
      and \(\lambda\) is called the <strong>co-state</strong> variable.
      
    
    </blockquote></i>
    <br>
  Notice that the multiplier (or co-state) \(\lambda(t)\), 
  unlike the Lagrangian multiplier in static optimisation, depends on time </strong><a href="#ref3">[3]</a>.
  <br>



  <br><br>

  <b>Theorem 6 — Pontryagin Maximum Principle</b><blockquote><i> 
    Assume the optimal control \(\mathbf{\gamma}^*\) is known and admissable (\(\mathbf{\gamma}^*\in\Gamma\)), 
    which generates the associated optimal trajectory \(\mathbf{x}^*\). 
    Then, there exists and adjoint trajectory \(\mathbf{p}^*\) conjugate to \(\mathbf{x}^*\) 
    such that
    <div style="overflow-x: auto; overflow-y: hidden;">
    \begin{equation}
    \begin{split}
    \dot{\mathbf{x}}(t) &= \nabla_\lambda H(\mathbf{x}^*, \mathbf{\gamma}^*, \lambda^*, t) \quad\quad\quad\quad \phantom{-} (\text{state equation})\\
    \dot{\lambda}(t) &= -\nabla_\mathbf{x} H(\mathbf{x}^*, \mathbf{\gamma}^*, \lambda^*, t) \quad\quad\quad\quad (\text{co-state equation})\\
     H(\mathbf{x}^*, \mathbf{\gamma}^*, \lambda^*, t) &\ge H(\mathbf{x}^*, \mathbf{\gamma}, \lambda^*, t), \quad \forall \gamma \in \Gamma \quad \phantom{-.} (\text{Pontryagin Maximum Principle})\\
     H(\mathbf{x}^*, \mathbf{\gamma}^*, \lambda^*, t) &= 0 \quad \forall t \in [0,T]\\
     \lambda^*(T) &= - \nabla_\mathbf{x}\phi(\mathbf{x}^*(T)) \quad\quad\quad\quad\quad\quad \phantom{-} (\text{Transversality condition})
    \end{split}
    \end{equation}     
    </div>

  </blockquote></i>
  <br>

  In other words, Pontryagin's Maximum Principle provides a necessary condition for optimality: 
  an optimal control must globally optimise the Hamiltonian.
  The state and co-state equations are also known as canonical equations and describe the system's dynamics. 
  <!-- Finally, the transversality condition states that the vector \(\lambda^*(T)\) 
  is orthogonal to the tangent space to \(\mathcal{S}\) at \(\mathbf{x}^*(T)\); 
  as such it represents a set of boundary conditions. 
  In particular, notice that -->



  <br><br>




<b>Example 1 — Minimal time for bringing a particle to rest at origin </b>
<br><br>
<i>
Consider the one-dimensional problem of bringing a particle to rest at \(x=0\) in minimal time.
The initial position and velocity of the particle are \(x_0\) and \(v_0\) respectively.

The control is the force \(f(t)\) applied to the particle, such that \(|f(t)|\le 1\).
<br>
Therefore, the problem is to minimise the total time 
\begin{equation}
\mathcal{T} = \int_0^T 1 dt,
\end{equation}
where \(T\) is the time when the particle reaches the origin, subject to the law of motion which,
recalling that \(v=\dot{x}\) and \(f=\dot{v}\), reads
\begin{equation}
 \frac{d}{dt} \begin{bmatrix} x \\ v  \end{bmatrix} = 
\left(\begin{matrix}
     0 & 1 \\
     0 & 0  
   \end{matrix}\right) \begin{bmatrix} x \\ v \end{bmatrix} 
   + \begin{bmatrix} 0 \\ 1 \end{bmatrix} f.
\end{equation}
<br>

To start off, one finds the Hamiltonian to be
<div style="overflow-x: auto; overflow-y: hidden;">
\begin{equation}
H = \mathbf{\lambda}^T\left(\begin{matrix}
0 & 1 \\
0 & 0  
\end{matrix}\right) \begin{bmatrix} x \\ v \end{bmatrix} 
+ \mathbf{\lambda}^T \begin{bmatrix} 0 \\ 1 \end{bmatrix} f - 1
= \lambda_1 v + \lambda_2 f - 1.
\label{double_integrator_hamiltonian}
\end{equation}
</div>

Because the term \(\lambda_2 f\) in the Hamiltonian is linear in \(f\), 
the maximum with respect to the control is achieved at one of the endpoints of the interval \([-1, 1]\), depending on the sign of \(\lambda_2\).
That is, the Hamiltonian is maximised by
<div style="overflow-x: auto; overflow-y: hidden;">
\begin{equation}
f^*=\begin{cases}
\text{sgn}\left(\lambda_2\right) & \text{if $\lambda_2 \ne 0$}\\
\text{any value} \in {-1,1} & \text{otherwise}
\end{cases} \quad.
\end{equation}
</div>
<br>

The costate variables are given by

\begin{equation}
\begin{split}
\dot{\lambda}_1 &= \partial_x H = 0, \\ 
\dot{\lambda}_2 &= \partial_v H = -\lambda_1.
\end{split}
\end{equation}

Denotining the values of the costate variables at termination by \(\bar{\lambda}_i = \lambda_i(t=T)\), 
we can therefore solve for the costate variables:

\begin{equation}
\begin{split}
\lambda_1 &= \bar{\lambda}_1, \\
\lambda_2 &= \bar{\lambda}_1 t + \bar{\lambda}_2.
\end{split}
\end{equation}

Here we observe that on the optimal trajectory, \(\lambda_2\) will switch sign at most once 
before termination at \(t_s=-\bar{\lambda}_2/\bar{\lambda}_1\), 
if \(\text{sgn}\left(\bar{\lambda}_1\right)\ne\text{sgn}\left(\bar{\lambda}_2\right)\). 
This means that the optimal control strategy consists of switching between maximal accelaration in one direction
to maximal acceleration in the opposite direction. This is therefore a bang-bang control.

<br>
Taking this result back to the equations of motion we find

<!-- dx = v -->
\begin{equation}
\begin{split}
\dot{v} &= f^* = \text{sgn}(\bar{\lambda}_1 t+ \bar{\lambda}_2) \\
\dot{x} &= v
\end{split}\quad,
\end{equation}

thus

\begin{equation}
\begin{split}
v &= \text{sgn}(\bar{\lambda}_1 t+ \bar{\lambda}_2) t + v_0 \\
x &= \frac{t^2}{2}\text{sgn}(\bar{\lambda}_1 t+ \bar{\lambda}_2) + v_0t + x_0 
\end{split}\quad.
\end{equation}

<br><br>

The terminal time \(T\) is free, therefore the transversality condition tells us \(H(t=T)=0\).
Hence, we can conclude that \(|\bar{\lambda}_2| = 1\).
<br>This allows one to find explicit solutions for \(x\) and \(v\). 
However, notice that one can also get rid of the parametrisation in \(t\) to find the associated orbits in the \(x\)-\(v\) phase plane. 
For \(f=\pm 1\)


  \begin{equation}
  \begin{split}
  x   &= \pm \frac{t^2}{2} + v_0t + x_0,\\
  v   &= \pm t + v_0,
  \end{split}
  \end{equation}

  or

  \begin{equation}
  x   = \pm \frac{v^2}{2} +(x_0\mp\frac{v_0}{2}).
  \end{equation}

  This family of parabulae intersects the origin only for \(v_0=\pm\sqrt{x_0}\) 
  which determines the switching curve \(x = -\frac{1}{2}|v|v\) (blue curve in Figure 1). Therefore, the optimal strategy is to apply \(f=1\) (\(f=-1\)) 
  if the initial condition lies below (above) the switching curve, follow the orbit up to the switching curve, 
  then switching the control value to  \(f=-1\) (\(f=1\)) and following the switching curve to the origin.


</i>

<figure style="text-align: center;justify-content: center;align-items: center;">
  <!-- Create a div where the graph will take place -->
  <div id="double_integrator_figure" style="width: 100%;"></div>
  <figcaption><i><b>Fig.1:</b> Optimal trajectory of the system described by equation \eqref{double_integrator_hamiltonian}, for different initial conditions (move your mouse to adjust them).
  </i></figcaption>
</figure>

<script src="./double_integrator.js"></script>

    The interested reader may find more details on this problem, also referred to as <i>double-integrator problem</i>, in <a href="#ref2">[2]</a>.
    
    <h3 style="text-align: left;"><b>Dynamic Programming</b></h3>

    A similar approach to the maximum principle developed during the cold war in the Soviet Union, 
    was developed in the same period in the United States. 
    However, while Pontryagin's maximum principle only yielded necessary conditions for optimality, 
    the <i>dynamic programming</i> approach pioneered by Bellman at RAND would yield both necessary and sufficient conditions for optimality.

    <br><br>
    Dynamic programming is a general approach: let us start by considering
    a discrete deterministic example. 
    We shall then move on to consider the continuous and stochastic extensions.

    <br>

    Consider a discrete system of the form 
    \begin{equation}
    x_{t+1} = f(x_t, \gamma_t), \quad t = 0, ..., T-1;
    \end{equation}
    here \(x_t\in X\subset\mathbb{N}\) and \(\gamma_t\in \Gamma \subset \mathbb{N}\).
     Furthermore, to each discrete step forward in time \(t\) is associated 
     the running payoff \(\ell(x_t, x_{t+1})\). Starting from \(x_0=0\), the objetive is to maximise 
     the total payoff.
    <br>
    The naive approach to solve this problem consists of enumerating each possible trajectory 
    starting from the initial state \(x_0\) up to time \(T\). 
    It's straightforward to find this approach implies evaluating \(|X|^T\) trajectories. 
    The evaluation of the payoff for each trajectory requires \(T\) operations, 
    so we denote by \(\mathcal{O}(T|X|^T)\) the computational complexity of this approach.
    <br>
    This is brute force search is depicted in Fig 2 left, where paths explored multiple times appear darker.
    <br>
    However, a better approach is possible. Let us first introduce an observation which will be crucial in the following.
    <br><br>
    <b>Definition 8 — Bellman's Principle of Optimality</b><blockquote><i> 

      For any point \(\tau>0\) on an optimal trajectory \(\mathbf{x}(t)\), 
      the remaining trajectory \(\mathbf{x}(t>\tau)\) is optimal 
      for the corresponding problem initiated at \(\tau\).
    
    </blockquote></i>
    <br>
    Or, in Bellman's own words <a href="#ref3">[3]</a>: 
    "An optimal policy has the property that whatever 
    the initial state and initial decision are, 
    the remaining decisions must constitute an optimal policy 
    with regard to the state resulting from the first decision."
    <br>
    Bellman's optimality principle therefore guarantees that from any point \(x_t\) on the optimal path,
    one can discard paths going backwards in time being certain that they are not portions of optimal trajectories. 
    <br>
    In order to leverage this intuition, let us thus define 
    at each time \(t\) the value function
     \begin{equation*}
     V(x_t) = \max\left(\sum_{s=t+1}^T \ell(x_s, x_s+1)\right).
     \end{equation*}
    Clearly this represents the largest payoff attainable starting from \(x_t\). 
    The introduction of \(V(x_t)\) is useful as it allows us to start at \(x_T\) and proceed backwards in time.
    At \(t=T\) terminal costs are known for all possible choices of \(x_T \in X\). 
    At \(t=T-1\), for each possible value \(x_{T-1} \in X\) we can compute the optimal payoff 
    as the sum of the one step running cost plus the terminal payoff, and record that.
    Then, one can repeat this procedure for each \( t < T - 1 \) up to \(t=0\). 
    Should one step involve two equivalent paths, either can be chosen at random.
    This process is depicted in Fig2, right.

     <figure style="text-align: center;justify-content: center;align-items: center;">
      <!-- Create a div where the graph will take place -->
      <div id="dp0_figure" style="width: 100%;"></div>
      <figcaption><i><b>Fig.2:</b> Brute force forward scheme (left) versus dynmamics programming (backward scheme) approach (right). 
        Darker paths are traversed more often.
        Here \(|X| = 3\) and \(T=5\).
      </i></figcaption>
    </figure>
    
    <script src="./dp0.js"></script>
    
    <br>
    At this point, for every \(x_t\) we have an associated value function, which imply two major achievements. 
    First, \(V(x_0)\)  represents the optimal payoff.
    Second, starting from \(x_0\) and proceeding greedily with respect to the value function we are therefore guaranteed
    to find the optimal path.
    <br>
    This dynamic programming <a href="#ref4">[4]</a> approach requires to compute the cost of a transition 
    and add it to the cost-to-go previously computed for each time \(t\), for each possible state in \(X\), 
    and for each control. 
    Hence, the computational complexity of the dynamic programming approach is \(\mathcal{O}(T|X||\Gamma|)\). 
    This is a major improvement compared to the naive forward scheme considered earlier!

    <br><br>

    The notion of dynamic programming can be formalised with the following theorem.
    <br><br>

    <b>Theorem 9 — Dynamic Programming</b><blockquote><i> 

      Given a Bolza problem over the time interval \([t, T]\), with payoff functional \(P[\gamma]\) 
      and subject to the law of motion \(\dot{x}(t)=f(x(t), \gamma(t), t)\),
      define the <b>Value Function</b>
      \begin{equation}
      V(x, t) := \sup_{\gamma_{[t, T]}}  P[\gamma].
      \end{equation}
      Then, one has
      \begin{equation}
      \label{eq:dynprogr}
      V(x, t) = \sup_{\gamma_{[t, T]}}  \left\{\int_t^{\tau}L(x(t'), \gamma(t'), t') dt' + V(x(\tau), \tau)\right\}.
      \end{equation}

      for any \(\tau\ge t\), and where \(x\) on the right hand side is he state trajectory corresponding to the control \(\gamma_{[t, T]}\).
    </blockquote></i>

    <br><br>

    <hr class="w3-opacity">

    <h6>Exercise 3</h6>
    Prove Theorem 9 on Dynamic Programming.
<br>
<button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 3</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
    Start by calling \(P_\tau\) the right hand side of \eqref{eq:dynprogr}:
    \begin{equation}
    P_\tau = \sup_{\gamma_{[t_0, T]}}  \left\{\int_t^{\tau}L(x(t), \gamma(t), t) dt + V(x(\tau), \tau)\right\}.
    \end{equation}
    For some \(\epsilon>0\), pick a policy \(\tilde{\gamma}\) such that
    \begin{equation}
      P[\tilde{\gamma}] \le V(x, t) + \epsilon.
    \end{equation}
    Under this control one has (by definition of the value function):
    \begin{equation}
      V(x(\tau), \tau) \le \int_\tau^T L(x(t), \tilde{\gamma}(t), t) dt + \phi(x(T)).
    \end{equation}
    Thus:
    \begin{equation}
    \begin{split}
      P_\tau &\le \int_s^\tau L(x(t), \tilde{\gamma}(t), t) dt + V(x(\tau), \tau)\\
             &\le \int_s^T L(x(t), \tilde{\gamma}(t), t) dt + \phi(x(T)) = P[\tilde{\gamma}] \le V(x, t) + \epsilon.
    \end{split}
    \end{equation}
    Now, consider instead a policy \(\hat{\gamma}\) such that
    \begin{equation}
      P_\tau + \epsilon \ge \int_s^\tau L(x(t), \hat{\gamma}, t)dt + V(x(\tau), \tau), 
    \end{equation}
    and a policy \(\check{\gamma}\) such that
    \begin{equation}
     V(x(\tau), \tau) + \epsilon \ge \int_\tau^T L(x(t), \check{\gamma}, t)dt. 
    \end{equation}

    FINISH

    <p align="right"> ◻</p>
</div>


<h6>Exercise 4</h6>
A number triangle is a sequence of numbers starting from a 1-digit number, followed by a 2-digit number, etc. For example:
<br>
<div style="text-align: center;justify-content: center;align-items: center;">
<p style="text-align: left;font-size: 15px; width: 300px;margin: 0 auto;">
  <normal style="color:red;">7</normal>
<br>
<normal style="color:red;">3</normal>&nbsp;8
<br>
<normal style="color:red;">8</normal>&nbsp;1&nbsp;0
<br>
2&nbsp;<normal style="color:red;">7</normal>&nbsp;4&nbsp;4
<br>
4&nbsp;<normal style="color:red;">5</normal>&nbsp;2&nbsp;6&nbsp;5
</p>
</div>
<br>

Write a function <code>Max_Path(T)</code> taking as input the number triangle 
<code>T</code> as a list of integers representing each level. 
The function should compute the highest sum of digits on a specific path starting at the top and ending somewhere at the base, following the rule that each step can only go directly-down or down-right.
In the example the maximising path is highlighted in red.

Test it on <code>T=[7,38,810,2744,45265]</code>: you should find <code>Max_Path(T)=30</code>.

Then, use the following function to generate a triangle of length <code>L</code>:
<pre><code class="lang-python">
  def gen_T(L):         
      from random import randint, seed
      seed(100)
      return [randint(10**(n) ,10**(n+1)-1) for n in range(L)]
    </code></pre>
 Test it for <code>L=50</code>: you should find <code>Max_Path(gen_T(50)) = 333</code>.


<br>
<button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 4</button>
<div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">

  <pre><code class="lang-python">
  # Sum lines from the bottom to the top and maximise sums
  
  def Max_Path(A):
      A = [[int(d) for d in str(n)] for n in A]  # List becomes list of lists
      while len(A) > 1:
          e1 = A[-1]                                   # Last level
          e2 = A[-2]                                   # Penultimate level
          s1 = [e1[n]  +e2[n] for n in range(len(e2))] # Sum below
          s2 = [e1[n+1]+e2[n] for n in range(len(e2))] # Sum below right
          MS = [max(a,b) for a,b in zip(s1,s2)]        # Max of possible sums
          A[-2] = MS                                   # MS in penultimate line
          A.pop()                                      # Remove last line
      return A[0][0]

</code></pre>




<p align="right"> ◻</p>
</div>

<hr class="w3-opacity">


    <h4 style="text-align: left;"><b> The Hamilton-Jacobi-Bellman equation</b></h4>

    Let us now consider an infinitesimal generalisation of Theorem 9.

    <br><br>

    <b>Theorem 10 — Hamilton-Jacobi-Bellman equation</b><blockquote><i> 
      Consider a value function defined by 
      
      \begin{equation*}
      V(x, t) := \sup_{\gamma_{[t_0, T]}}  P[\gamma].
      \end{equation*}

      Then, \(V\) is the unique viscosity solution [<a href="#ref6">6</a>, <a href="#ref7">7</a>] of the Hamilton-Jacobi-Bellman equation

      \begin{equation}
      -\partial_t V(x, t)  = \max_{\gamma} \left\{ L(x, \gamma, t)       
        + \nabla_x V(x, t) \cdot f(x, \gamma, t)
        \right\},
      \end{equation}
        with \(V(x, T) = \phi(x(T))\).
        Moreover, this provides a necessary and sufficient condition for optimality.
    </blockquote></i>

    <br>

    <hr class="w3-opacity">

    <h6>Exercise 5</h6>
    Derive the Hamilton-Jacobi-Bellman equation from Theorem 10.
<br>
<button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 5</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
      To start, consider an infinitesimal dynamic programming principle
      \begin{equation}
      \label{eq:dpp_eq}
      V(x, t) = \sup_{\gamma_{[t, t+\delta t]}}  \left\{\int_t^{t+\delta t}L(x(t), \gamma(t), t) dt + V(x(t+\delta t), t+\delta t)\right\}.
      \end{equation}
      A Taylor expansion of the equation of motion \(\dot{x}(t) = f(x(t), \gamma(t), t)\) gives
      \begin{equation}
      \begin{split}
      x(t+\delta t) &= x(t) + \int_t^{t+\delta t} f(x(t'), \gamma(t'), t') dt'\\
                    &= x(t) + f(x(t), \gamma(t), t) \delta t +\mathcal{O}(\delta t).
      \end{split}
      \end{equation}
      Expanding further \eqref{eq:dpp_eq} to first order in \(\delta t\), 
      and assuming sufficient regularity of \(V\), gives
      \begin{equation}
      \begin{split}
      V(x, t) &= \sup_{\gamma_{[t, t+\delta t]}} \left\{ L(x(t), \gamma(t), t)\delta t 
                             + V(x, t) +\partial_t V(x, t) \delta t 
                             + \nabla_x V(x, t) \cdot f(x(t), \gamma(t), t) \delta t 
                             \right\}\\
            0 &= \sup_{\gamma_{[t, t+\delta t]}} \left\{ L(x(t), \gamma(t), t)\delta t 
                              +\partial_t V(x, t) \delta t 
                              + \nabla_x V(x, t) \cdot f(x(t), \gamma(t), t) \delta t
                              \right\},
      \end{split}
      \end{equation}
  
      which after rearranging and taking the limit \(\delta t\rightarrow 0\) finally yields
      \begin{equation}
      -\partial_t V(x, t)  = \sup_{\gamma} \left\{ L(x, \gamma, t)       
                              + \nabla_x V(x, t) \cdot f(x, \gamma, t)
                              \right\}.
      \end{equation}
    <p align="right"> ◻</p>
</div>

    <hr class="w3-opacity">

<br>

<h4 style="text-align: left;"><b> HJB vs. Maximum Principle</b></h4>

...
<br>

<h4 style="text-align: left;"><b> Linear-Quadratic Regulator</b></h4>

Let us now turn our attention to the specialised cased 
of a finite-horizon linear system with quadratic payoff:


\begin{equation}
\begin{split}
f(x(t), \gamma(t), t) &= A(t)x + B(t)\gamma,\\
L(x(t), \gamma(t)) &= Q(t)x^2(t)+R(t)\gamma^2(t),\\
\phi(x(T)) &= Mx^2(T).,
\end{split}
\end{equation}
with \(A\), \(B\), \(Q\), \(R\), and \(M\) all positive.
That is to say, the system with equation of motion
\begin{equation}
\dot{x} = A(t)x + B(t)\gamma,
\end{equation}

and payoff function
\begin{equation}
P[\gamma] = \int_0^T Q(t)x^2(t)+R(t)\gamma^2(t)dt + Mx^2(T).
\end{equation}
As we shall see, this linear-quadratic regulator (LQR)
is particularly useful as it is fully solvable and allow us to discuss
controllability and observability...

<br>
The Hamiltonian for the system can be then found to be
\begin{equation}
\label{eq:lqr_hamiltonian}
H(x, \gamma, \lambda, t) = A(t)\lambda x+B(t)\lambda\gamma-Q(t)x^2-R(t)\gamma^2,
\end{equation}


<b>Theorem 11 — LQR Linear State Feedback Law</b><blockquote><i> 
  For the Linear Quadratic Regulator defined above, 
  one has the following <i>linear state feedback law</i>
  
  \begin{equation}
  \label{eq:lqr_lsfl}
  \gamma^{*}(t) = -R^{-1}(t)B(t)\chi(t) x^{*}(t),
  \end{equation}
  where the function \(\chi(t)\) satisfied the <i>Riccati Differential Equation</i>
  \begin{equation}
  \label{eq:lqr_rde}
  \dot{\chi} = R^{-1}B^2\chi^2-2A\chi-Q,
  \end{equation}
  with boundary condition \(\chi(T)=M\).
  Furthermore
  \begin{equation}
  \label{eq:lqr_state_costate_linear_rel}
  \lambda^*(t) = -2\chi(t) x^*(t).
  \end{equation}
</blockquote></i>

<br>
<button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Proof of Theorem 11</button>
<div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
  The gradient of the Hamiltonian \eqref{eq:lqr_hamiltonian} with respect to the control is

  \begin{equation}
  \partial_\gamma H = B(t)\lambda-2R(t)\gamma,
  \end{equation}
  
  and \(\partial_\gamma^2H=-2R(t)<0\). 
  Therefore, an optimal control \(\gamma^*\) must satisfy
  
  \begin{equation}
  \label{eq:lqr_costate_optimal_condition}
  \gamma^*= \frac{R^{-1}(t)}{2}B(t)\lambda^*,
  \end{equation}
  
  while the co-state satisfies the adjoin equation
  
  \begin{equation}
  \begin{split}
  \dot{\lambda}^* &= -\partial_x H = 2Q(t)x^*-A(t)\lambda^* ,\\
  \lambda^*(T) &= Mx^*(T).
  \end{split}
  \end{equation}
  
  For the whole system we can therefore write the canonical equations
  
  \begin{equation}
  \label{eq:lqr_canonical_equations}
  \begin{pmatrix}
      \dot{x}^*\\
      \dot{\lambda}^*
    \end{pmatrix} =
    
  \mathcal{H}(t)
    \begin{pmatrix}
      x^*\\
      \lambda^*
    \end{pmatrix}
  \end{equation}
  
  where the so called Hamiltonian matrix \(\mathcal{H}(t)\)
  is given by
  \begin{equation}
    
  \mathcal{H}(t) = 
    \begin{pmatrix}
      A(t) & \frac{R^{-1}(t)}{2}B^2(t)\\
      2Q(t) & -A(t)
    \end{pmatrix}. 
  \end{equation}
  
  
  Consider now instead the transition matrix, or <i> propagator </i> \(P(t, t_0) = P^{-1}(t_0, t)\)
  such that
  \begin{equation}
  \label{eq:propagated}
  \begin{pmatrix}
  x^*(t)\\
  \lambda^*(t)
  \end{pmatrix} = P(t, t0)\begin{pmatrix}
  x^*(t_0)\\
  \lambda^*(t_0)
  \end{pmatrix}.
  
  \end{equation}
  
  The transversality condition \(\lambda^*(T) = Mx^*(T)\) then implies that \eqref{eq:propagated}
  can be rewritten as 
  
  \begin{equation}
  \label{eq:propagated2}
  \begin{pmatrix}
  x^*(t)\\
  \lambda^*(t)
  \end{pmatrix} = \bar{P}(t, t0)\begin{pmatrix}
  x^*(t_0)\\
  x^*(t_0)
  \end{pmatrix},
  \end{equation}
  so that we can write
  
  \begin{equation}
  \lambda^* = -2\chi x^*
  \end{equation}
   
  with 
  
  \begin{equation}
  -2\chi = \frac{\bar{P}_{21}+\bar{P}_{22}}{\bar{P}_{11}+\bar{P}_{12}},
  \end{equation}
  
  where \(\bar{P}_{ij}\) denotes the element \(i,j\) of \(\bar{P}\).
  <br>
  This proves \eqref{eq:lqr_state_costate_linear_rel}.
  <br>
  Combining \eqref{eq:lqr_costate_optimal_condition} with \eqref{eq:lqr_state_costate_linear_rel}
  further yields \eqref{eq:lqr_lsfl}.
  <br>
  We are left to prove that \(\chi\) satisfies the Riccati equation \eqref{eq:lqr_rde}.
  Differentiating \eqref{eq:lqr_state_costate_linear_rel} yields
  \begin{equation}
  \dot{\lambda}^* = -2\dot{\chi} x^* -2\chi \dot{x}^*.
  \end{equation}
  Expanding \(\dot{x}^*\) and \(\dot{\lambda}^*\) via the canonical equations \eqref{eq:lqr_canonical_equations} shows that
  \begin{equation}
  2Q x^* - A\lambda^* = -2\dot{\chi} x^* -2\chi Ax^* -2\chi \frac{R^{-1}}{2}B^2\lambda^*.
  \end{equation}
  Further expanding \(\lambda^*\) via \eqref{eq:lqr_state_costate_linear_rel} yields
  the desired result.
  <br>
  Finally, the boundary condition for the Riccati equation 
  can be found by combining \eqref{eq:lqr_state_costate_linear_rel}
  with the boundary condition \(\lambda^*(T) = -2Mx^*(T)\) for the co-state variable.
  
<p align="right"> ◻</p>
</div>

<br><br>


<b>Theorem 12 — Global optimality of the Linear State Feedback Law</b><blockquote><i> 
  The <i>linear state feedback law</i> \eqref{eq:lqr_lsfl}
  obtained by the maximum principle is globally optimal.

</blockquote></i>

<button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Proof of Theorem 12</button>
<div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
  The 
<p align="right"> ◻</p>
</div>

<br>



Theorems 11 and 12 are remarkable results.
They make the LQR problem substantially more tractable 
than the general optimal control problem \eqref{xxx}. 
Crucially, the task of solving the HJB partial differential equation
is reduced to solving the Riccati differential equation wich is an ODE.


<br>
<br>
<hr class="w3-opacity">

<h6>Exercise 6</h6>
Consider the integrator \(\dot{x}=\gamma\) with payoff \(P(\gamma) = -\int_0^Tx^2(t)+\gamma^2(t) dt\). 
Find the optimal feedback law for the control.
<br>
<button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 5</button>
<div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
  To start, notice that the problem corresponds to the following identification
  \begin{equation*}
  A = M = 0,\quad B=Q=R=1.
  \end{equation*}
  The Riccati equation is therefore
  \begin{equation*}
  \dot{\chi}=\chi^2-1,
  \end{equation*}
  with boundary condition \(\chi(T)=0\). Thus,
  \begin{equation*}
  \begin{split}
  \int_{\chi(t)}^0\frac{d\chi}{\chi^2-1}&=\int_t^T ds,\\
  \tanh^{-1}(\chi(t))&=T-t,\\
  \tanh{T-t}&=\chi(t).
  \end{split}
  \end{equation*}
  Therefore the optimal control is given by the feedback law
  \begin{equation*}
  \gamma(t)=-\tanh(T-t)x(t).
  \end{equation*}
  <p align="right"> ◻</p>
</div>

<hr class="w3-opacity">

<br>
















<br>

    <h2 style="text-align: left;"><b> Stochastic Control</b></h2>

    While so far we have discussed deterministic control problems, 
    many real world applications require the control of noisy processes, 
    ranging from the execution of optimal trading strategy, resources allocation, 
    or the control of spacecrafts and robots.
<br>
    Let us therefore introduce the stochastic extensions of the notions learnt above.

    <br><br>
    <b>Definition 11 — Stochastic Optimal Control Problem</b><blockquote><i> 

      Given a stochastic process defined by the SDE
      \begin{equation}
      \label{eq:canonical_sde}
      dX(t) = f(X(t), \gamma(t), t)dt+\sigma(X(t), \gamma(t), t)dW(t),
      \end{equation} 
      and the associated payoff
      \begin{equation}
      \mathbf{P}[\gamma] = \mathbb{E}\left[\int_0^T L(X(t), \gamma(t), t)dt +\phi(X(T))\right],
      \end{equation} 
      the stochastic Bolza problem is 
      \begin{equation}
      \sup_{\gamma\in\Gamma}\mathbf{P}[\gamma],
      \end{equation}
      subject to \eqref{eq:canonical_sde}, with the set of admissable controls \(\Gamma\)
      being adapted to \(W\).
    </blockquote></i>
Clearly this reduces to the deterministic Bolza problem for \(\sigma=0\).
    

<br><br>

<b>Theorem 12 — Stochastic Dynamic Programming</b><blockquote><i> 

  Given a stochastic Bolza problem over the time interval \([t, T]\), with payoff functional \(P[\gamma]\)
  define the <b>Value Function</b>
  \begin{equation}
  V(x, t) := \sup_{\gamma_{[t, T]}}  \mathbb{E}\left[\int_t^T L(X(t'), \gamma(t'), t')dt' +\phi(X(T))\right],
  \end{equation}
  with \(X(t)=x\).
  Then, one has
  \begin{equation}
  \label{eq:stochdynprogr}
  V(x, t) = \sup_{\gamma_{[t, T]}}  \mathbb{E}\left\{\int_t^{\tau}L(X(t'), \gamma(t'), t') dt + V(X(\tau), \tau)\right\}.
  \end{equation}

  for any stopping time \(\tau\ge t\), .
</blockquote></i>

<br><br>



    <b>Theorem 13 — Stochastic Hamilton-Jacobi-Bellman equation</b><blockquote><i> 

    The value function is the unique viscosity solution to the stochastic Hamilton-Jacobi-Bellman equation
    \begin{equation}
    -\partial_t V(x, t)  = \sup_{\gamma} \left\{ L(x, \gamma, t)       
      + \nabla_x V(x, t) \cdot f(x, \gamma, t) 
      + \frac{1}{2}\text{Tr}\left[\sigma^T(x, \gamma, t)\nabla_x^2V(x, t)\sigma(x, \gamma, t)\right]
      \right\},
    \end{equation}
      with \(V(X, T) = \phi(X(T))\).
  </blockquote></i>

    <br><br>



  


    <hr class="w3-opacity">

    <h6>Exercise 7</h6>
    Derive the Stochastic Hamilton-Jacobi-Bellman equation from Theorem 13. 
    <br>
    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 1</button>
        <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
        
    
        <p align="right"> ◻</p>
    </div>
    
    <h6>Exercise 8</h6>
    Show that a p
    <br>
    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 2</button>
        <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
        Consider 
    
    
        <p align="right"> ◻</p>
    </div>
    <hr class="w3-opacity">

    <br>

    <b>Example 2 — Optimal Consumption-Investment </b>
    <br><br>
    <i>
    Consider the stochastic optimal control problem of adjusting consumption \(C_t\) and investments in an initial amount of wealth \(W_0\) in either a saving account with interest rate \(r\)
    or in a risky portfolio with average rate of return \(\mu\) and return variance \(\sigma^2\):
    
    \begin{equation}
    \frac{d S_t}{S_t} = r dt,\quad\quad \frac{d P_t}{P_t} = \mu dt + \sigma dZ_t,
    \end{equation}

    with \(dZ_t\) a standard Wiener process. 
    
    The utility of consumption is \(U(c) = \log(c)\) and the rate of discount applied to consumption utility is \(\rho\).
    <br>
    Denoting by \(Q_t\) the fraction of wealth invested in the risky portfolio at time \(t\),
    the time-dynamics of wealth is therefore described by<a href="#ref9">[9]</a>
    \begin{equation}
    dW_t =  Q_tW_t(\mu dt+\sigma dZ_t) + (1-Q_t)W_t r dt  - C_tdt .
    \end{equation}
    That is, the change in wealth is driven by the return on the risky portfolio, 
    plus the return on the saving account, minus the amount of wealth consumed.
    <br>
    Therefore, the optimal control problem is 

      \begin{equation}
      \begin{split}
      \text{max}&\left\{\mathbb{E}\int_0^T e^{-\rho t}U(C_t) dt\right\} \\ 
      & \\
      & \text{subject to}\\
      & \\
      dW_t &=  Q_tW_t(\mu dt+\sigma dZ_t) + (1-Q_t)W_t r dt  - C_tdt\\
      C_t &\ge 0      
      \end{split}
      \end{equation}
    </i>




<br><br>
  <h4 id="REFERENCES"><i>References and notes</i></h4>

  [1] <a href="https://link.springer.com/article/10.1007/BF02419594" id="ref1">"Über zwei Euler’sche Aufgaben aus der Variationsrechnung", Oskar Bolza, 1913, Annali di Matematica Pura ed Applicata</a>
  <br><br>
  [2] <a href="https://arxiv.org/pdf/1909.03192.pdf" id="ref2">"Analytic Solution of the Time-Optimal Control of a Double Integrator from an Arbitrary State to the State-space Origin", Marcello Romano, and Fabio Curti, 2019. </a>
  <br><br>
  [3] <a href="https://gwern.net/doc/statistics/decision/1957-bellman-dynamicprogramming.pdf" id="ref3">"Dynamic Programming", Richard Bellman, 1957, Chap. III.3. </a>
  <br><br>
  [4] <normal id="ref4">Notice that while related, the Control Hamiltonian of optimal control theory is distinct from the Hamiltonian used in mechanics. 
    In particular, the latter is used to derive the equation of motion of a dynamical system. 
    The Hamiltonian of optimal control theory instead, is a concept developed by Lev Pontryagin, 
    with the purpose to provide conditions for extrimising a functional with respect to a control variable. <normal>
  <br><br>
  [5] <normal id="ref5">Why is this approach named <i>Dynamic Programming</i>?
    The term was coined in the early 50s by Richard Bellman while working on his research on multistage decision processes 
    at RAND Corporation, at that time fully funded by the US government. 
    The justification for the label has a rather interesting story. 
    From Bellman's autobiography <i>Eye of the Hurricane: An Autobiography</i> (1984, page 159): 
    «<i>I spent the Fall quarter (of 1950) at RAND. 
      My first task was to find a name for multistage decision processes. 
      An interesting question is, 
      "Where did the name, dynamic programming, come from?" 
      The 1950s were not good years for mathematical research. 
      We had a very interesting gentleman in Washington named Wilson.
      He was Secretary of Defense, and he actually had a pathological fear and hatred of the word "research". 
      I'm not using the term lightly; I'm using it precisely. 
      His face would suffuse, he would turn red, 
      and he would get violent if people used the term research in his presence. 
      You can imagine how he felt, then, about the term mathematical. 
      The RAND Corporation was employed by the Air Force, and the Air Force had Wilson as its boss, 
      essentially. Hence, I felt I had to do something to shield Wilson and the Air Force 
      from the fact that I was really doing mathematics inside the RAND Corporation. 
      What title, what name, could I choose? 
      In the first place I was interested in planning, in decision making, in thinking. 
      But planning, is not a good word for various reasons. 
      I decided therefore to use the word "programming". 
      I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying. 
      I thought, let's kill two birds with one stone. 
      Let's take a word that has an absolutely precise meaning, namely dynamic, 
      in the classical physical sense. 
      It also has a very interesting property as an adjective, 
      and that is it's impossible to use the word dynamic in a pejorative sense. 
      Try thinking of some combination that will possibly give it a pejorative meaning. 
      It's impossible. Thus, I thought dynamic programming was a good name. 
      It was something not even a Congressman could object to. 
      So I used it as an umbrella for my activities.</i>»
    <normal>
      <br><br>
      [6] <normal id="ref6">Viscosity solutions are a class of weak solutions to non-linear PDEs,
        which can be seen as the limit of solutions of the original PDE 
        regularised with a diffusive term (from which the term "viscosity"). 
        A viscosity solution does not necessarily satisfy the PDE at every point like a classical solution.
        Instead, it satisfies certain inequalities that capture the essence of the PDE.
        See e.g. <a href="#ref7">[7]</a>. 
        <normal>
          <br><br>
      [7] <normal id="ref7"><a href="http://home.ustc.edu.cn/~wclw8181/wffc.files/Partial%20Differential%20Equations.Evans.pdf">L. C. Evans, Partial Differential Equations, AMS, 2010</a>. 
        <normal>
          <br><br>
      [8] <normal id="ref8">"Calculus of Variations and Optimal Control Theory - A concise Introduction", D. Liberzon, 2009.
            <normal>
      [9] <normal id="ref9">This is of course under the assumption of continuous trading and absence of transaction and commission costs. See e.g.  <a href="https://www.sciencedirect.com/science/article/pii/0304414981900260">Harrison and Pliska, "Martingales and stochastic integrals in the theory of continuous trading", Stochastic Processes and their Applications, (1981)</a>  for a more rigorous treatment of the problem.
            <normal>
      <br>

<!--  https://bookdown.org/egarpor/NP-UC3M/kde-ii-asymp.html-->
<!--  https://bookdown.org/egarpor/NP-EAFIT/intro-nonpar.html-->

<!--  https://bookdown.org/egarpor/inference/-->
<!--  https://bookdown.org/egarpor/PM-UC3M/-->


  <br><br>

  <!-- ________________________________________________________________________________________________ -->
  <!-- ________________________________________________________________________________________________ -->
  <!-- ________________________________________________________________________________________________ -->
  <!--    <h7>-->
  <!-- <span style="float:right;">
 <a href="./L2.html"><b><img src="../Icons/next.png" width="20px" align="right">Next</b></a>
 </span>
 </h7>
 <br> -->

  <h5> <a href="/Teaching.html"><b><img src="/Icons/back.png" width="20px"> Back to Teaching</b></a></h5>



  <hr class="w3-opacity">
</div>
</div>
<script type="text/javascript" src="/js/footer.js"></script>

<!-- End page content -->
</div>

<script src="/js/OpenCloseSidebar.js"></script>
<!--<script src="../js/ModalImageGallery.js"></script>-->
<script src="/js/OpenCloseCollapsible.js"></script>

</body>
</html>
