<!DOCTYPE html>
<html>
<head>
    <title>Luca Mingarelli</title>
    <link rel="icon" href="../Icons/stork.png" type="image/png">
    <style>
        .collapsible {background-color: #777;color: white;cursor: pointer;  padding: 2px;width: 100%;
            border: none;  text-align: left;  outline: none;  font-size: 15px; border-radius: 2px;}
        /*.active,*/
        .collapsible:hover {  background-color: #555;}
        .content {display: none;  overflow: hidden;  background-color: #DCDCDC;}
    </style>
</head>

<!--<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>-->
<script>window.MathJax = {tex: {tags: 'ams'}};</script>
<script id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<!--<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG"></script>-->

<link rel="stylesheet" href="prism/prism.css"> <!-- For code highlight -->
<script src="prism/prism.js"></script>         <!-- For code highlight -->


<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="../css/w3.css">
<link rel="stylesheet" href="../css/font.css">
<link rel="stylesheet" href="../css/responsiveiframe.css">
<script src="../js/style_preamble.js"></script>

<body class="w3-light-grey w3-content" style="max-width:2600px">
<!-- NavigationBar -->
<script src="js/Navbar.js"></script>
<!-- _____________ -->
<!-- PRE_CONTENT -->
<script src="js/pre_content.js"></script>
<!-- _____________ -->




<div class="w3-content w3-justify" style="max-width:800px">

    <h1><b>Copulae — Concepts and examples</b></h1>




    <h2>Definitions and basic properties</h2>

<b>Definition 1 — Copula</b><blockquote><i> A \(d\)-dimensional copula is a multivariate cumulative density function
    \(C:[0,1]^d\rightarrow[0,1]\) with marginals which are uniformly distributed on the unit interval:
    \begin{equation}
    C(u_1, ..., u_d)=\mathbb{P}(U_1\le u_1, ..., U_d\le u_d),
    \end{equation}
    where \(U_j\sim\mathcal{U}[0,1], \ \forall j\).</i>
    </blockquote>
    <br>

    <b>Definition 2 — Copula density</b><blockquote><i>
    Given a differentiable copula \(C\) the copula density \(c\) is defined by:
    \begin{equation}
    c(u_1, ..., u_d) = \frac{\partial^dC(u_1, ..., u_d)}{\partial u_1...\partial u_d}.
    \label{eq:copula_density}
    \end{equation}
</i></blockquote>

    It follows that \(C(u_1, ..., u_d)\) is monotonically
    increasing in each component \(u_i=C(1,...,u_i, ..., 1)\).
    Moreover, the copula function \(C\) is isotonic,
    that is \(u_j\le v_j\ \forall j \implies C(u_1, ..., u_d)\le C(v_1, ..., v_d)\).
    Finally, notice that (under mild regularity conditions) one has the following proposition.
<br><br>
    <b>Proposition 3</b><blockquote><i> Given a \(d\)-dimensional copula \(C\) one obtains
    the conditional cumulative density function
    \(F(\mathbf{u}|U_j=u_j)=\mathbb{P}(U_1\le u_1, ..., U_d\le u_d\ |\ U_j=u_j) \) as
    \begin{equation}
    F(\mathbf{u}|U_j=u_j)=\frac{\partial}{\partial u_j} C(u_1, ..., u_d).
    \label{eq:conditional_copula}
    \end{equation}
</i></blockquote>


    <br>
    In order to better understand the significance of <i>Definition 1</i>
    let us consider a two-dimensional random vector \((X_1, X_2)\)
    with \(X_j\sim F_j\) and denoting the joint CDF by \(F\).
    Applying the <i><a href="https://en.wikipedia.org/wiki/Probability_integral_transform">probability integral transform</a></i>
    one can find two marginals \(U_j=F_j(X_j)\)
    uniform on the unit interval so that one can write
    \begin{equation}
    F(x_1, x_2)=C(F_1(x_1), F_2(x_2)).
    \label{eq1}
    \end{equation}

    Therefore, a <i>copula</i> is the recipe needed to link
    the marginals to the joint distribution.


<hr class="w3-opacity">

    <h6>Exercise 1</h6>
    Show that \(U_j=F_j(X_j)\) implies \(U_j\sim\mathcal{U}[0,1]\).
    <br>
    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 1</button>
        <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
        Start by considering the CDF \(F_{U_j}\) of \(U_j\). Then:
        \begin{align*}
        F_{U_j}(u_j) &= \mathbb{P}(U_j\le u_j) \newline
        &= \mathbb{P}(F_j(X_j)\le u_j) \newline
        &= \mathbb{P}(X_j\le F_j^{-1}(u_j)) \newline
        &= F_j(F_j^{-1}(u_j)) = u_j.
        \end{align*}
Because \(F_j(X_j)\in [0, 1]\) one can see that \(F_{U_j}(u_j) = u_j\) is the CDF of a
        uniform distribution on the unit interval.
            Therefore \(U_j\sim\mathcal{U}[0,1]\).
        <p align="right"> ◻</p>
    </div>

    <h6>Exercise 2</h6>
    Show that equation \eqref{eq1} holds.

    <br>
    <button class="collapsible" style="margin-bottom: 10px; margin-top: 10px">Solution 2</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">

        Notice that
        \begin{align*}
        F(x_1, x_2) &= \mathbb{P}(X_1 \leq x_1, X_2 \leq x_2) \newline
        &= \mathbb{P}(U_1 \leq F_1(x_1), U_2 \leq F_2(x_2)) \newline
        &= C(F_1(x_1), F_2(x_2)).
        \end{align*}

        <p align="right"> ◻</p>
    </div>

    <h6>Exercise 3</h6>
    Consider two uniform random variables \(U_1\) and \(U_2\) with known differentiable copula \(C\).
    Assuming \(U_1=u_1\) is observed, prove \eqref{eq:conditional_copula} in <i>Proposition 1</i>
    for this \(d=2\) case.

    <br>
    <button class="collapsible" style="margin-bottom: 10px; margin-top: 10px">Solution 3</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">

        We have:
        \begin{align*}
        \mathbb{P}(U_2\le u_2\ |\ U_1=u_1) &= \lim_{\epsilon\rightarrow 0}\frac{\mathbb{P}(U_2\le u_2,\ U_1\in\ ]u_1-\epsilon, u_1+\epsilon])}{\mathbb{P}(U_1\in\ ]u_1-\epsilon, u_1+\epsilon])} \newline
        &= \lim_{\epsilon\rightarrow 0}\frac{C(u_1+\epsilon,u_2)-C(u_1-\epsilon,u_2)}{2\epsilon} \newline
        &= \frac{\partial}{\partial u_1}C(u_1, u_2).
        \end{align*}

        <p align="right"> ◻</p>
    </div>

<hr class="w3-opacity">

    <br>
Equation \eqref{eq1} can be formalised into the following theorem.
    <br><br>
    <b>Theorem 4 — Sklar</b><blockquote><i> Given a \(d\)-dimensional cumulative density function \(F\), with marginals \(F_j\),
    there exists a copula \(C\) such that
    \begin{equation}
    F(x_1, ..., x_d) = C(F_1(x_1), ..., F_d(x_d)),
    \label{eq:sklar}
    \end{equation}
    for all \(x_j\in\mathbb{R}\). Similarly, given a copula \(C\)
    and a set of \(d\) univariate cumulative density functions labelled \(F_1, ..., F_d\),
    equation \eqref{eq:sklar} uniquely defines a joint multivariate
    cumulative density function \(F\) with marginals \(F_1, ..., F_d\).
</i></blockquote>

    <br>
    One has the following corollaries.

    <br><br>
    <b>Corollary 4.1</b><blockquote><i> From Sklar's theorem it follows that given a joint
    cumulative density function \(F\) and its marginals, the copula is defined by
    \begin{equation}
    C(u_1, .., u_d) = F\left(F^{-1}_1(u_1), ..., F^{-1}_d(u_d)\right),
    \end{equation}
    where \(F^{-1}_j(u_j) = \inf\{x: F_j(x)\ge u_j\}\).
</i></blockquote>
    <br>
    <b>Corollary 4.2</b><blockquote><i> From Sklar's theorem it follows that given the
    marginal cumulative density functions \(F_i\) and the associated
    marginal probability density functions \(f_i\),
    the joint probability density function \(f\) can be written as
    \begin{equation}
    f(x_1, .., x_d) = c\left(F_1(x_1), ..., F_d(x_d)\right)\prod_{i=1}^df_i(x_i),
    \end{equation}
    where \(c\) is the copula density defined in \eqref{eq:copula_density}.
</i></blockquote>


    <h3>Fundamental copulae</h3>

    Let us now consider three fundamental examples corresponding to the cases of
    perfect dependence and independence. As we shall discuss,
    these are extreme cases which provide bounds to any other copula, and constitute therefore
    preliminary definitions for some results which will be presented later.
    First we consider the case of independent random variables.
<br><br>
    <b>Definition 5 — Independence copula</b><blockquote><i>
    The joint distribution of \(d\) independent random variables is defined by the
    independence copula
    \begin{equation}
    C_{\text{ind}}(u_1, ..., u_d) = \prod_{i=1}^d u_i.
    \label{eq:independence_copula}
    \end{equation}
</i></blockquote>
When all the random variables considered are deterministically related \(X_j = T_{ji}(X_i)\)
    for some set of strictly increasing transformations \(T_{ji}=T_{ij}^{-1}\), one can derive a relation between each pair
    of cumulative density functions as
    \[F_i(x) = \mathbb{P}(X_i\le x)
             = \mathbb{P}(T_{ji}(X_i)\le T_{ji}(x))
             = \mathbb{P}(X_j\le T_{ji}(x))
             = F_j(T_{ji}(x)),\]
    which for all \(i,j\) implies
    \[U_i = F_i(X_i) = F_j(T_{ji}(X_i)) = F_j(X_j) = U_j.\]

    Similarly, consider two random variables such that
    \(X_2=D(X_1)\) for any strictly decreasing transformation \(D\) one has
    \[F_1(x) = \mathbb{P}(X_1 \leq x) = \mathbb{P}(D(X_1) \geq D(x))
    = \mathbb{P}(X_2 \geq D(x)) = 1 - F_2(D(x)),\]
    which implies
    \[U_1 = F_1(X_1) = 1-F_2(D(X_1)) = 1-F_2(X_2) = 1-U_2.\]


    This motivates the definition of the following <i>comonotonicity</i> and <i>counter-monotonicity</i> copulae.
    <br><br>
    <b>Definition 6 — Comonotonicity copula</b><blockquote><i>
    The copula associated with \(d\) equal uniformly distribute variables \(U_i=U_j,\ \forall\ i,j\),
    is called a comonotonicity copula and takes the form
    \begin{equation}
    C_{\text{co}}(u_1, ..., u_d) = \min\{u_1, ..., u_d\}.
    \label{eq:comonotonicity_copula}
    \end{equation}
</i></blockquote>

    <b>Definition 7 — Counter-monotonicity function</b><blockquote><i>
    The copula associated with \(d=2\) uniformly distributed random variables \(U_2=1-U_1\)
    is called a counter-monotonicity copula as takes the form
    \begin{equation}
    W_{\text{counter}}(u_1, u_2) = \max\{u_1 + u_2 -1, 0\}.
    \label{eq:counter_monotonicity_copula}
    \end{equation}
    A generalisation of \eqref{eq:counter_monotonicity_copula} to arbitrary \(d\) is given
    by the counter-monotonicity function
    \begin{equation}
    W_{\text{counter}}(u_1, ..., u_d) = \max\{\sum_{i=1}^d u_i - d + 1, 0\};
    \label{eq:counter_monotonicity_function}
    \end{equation}
    notice however that \eqref{eq:counter_monotonicity_function} is not a copula.

</i></blockquote>
    <br>


    <hr class="w3-opacity">
    <h6>Exercise 4</h6>
    Show that given two independent random variables \(X_1\) and \(X_2\)
    the associated joint multivariate cumulative density function takes
    indeed the form \eqref{eq:independence_copula} from <i>Definition 3</i>.
    <br>
    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 4</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
        The joint multivariate cumulative density funtion is \(F(x_1, x_2) = C(F_1(x_1), F_2(x_2))\). The independence of the two random variables \(X_1\perp\!\!\!\perp X_2\)
        also implies the associated uniform random variables \(U_j=F_j(X_j)\) are also independent:
        \(U_1 \perp\!\!\!\perp U_2\).
        Therefore, one has:
        \begin{align*}
        C(u_1, u_2) &= \mathbb{P}(U_1 \leq u_1, U_2 \leq u_2) \newline
                    &= \mathbb{P}(U_1 \leq u_1) \mathbb{P}(U_2 \leq u_2) \newline
                    &= u_1 u_2.
        \end{align*}
        Hence, \(F(x_1, x_2) = F_1(x_1)F_2(x_2)\).
        <p align="right"> ◻</p>
    </div>

    <h6>Exercise 5</h6>
    Show that <i>Definition 6</i> indeed is the copula associated with perfectly correlated random variables
    \(U_1=U_2\) for the case \(d=2\).
    <br>
    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 5</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
        Notice that
        \begin{align*}
        C(u_1, u_2) &= \mathbb{P}(U_1 \leq u_1, U_2 \leq u_2) \newline
        &= \mathbb{P}(U_1 \leq u_1, U_1 \leq u_2) \newline
        &= \mathbb{P}(U_1 \leq \min \{u_1, u_2\}) \newline
        &= \min \{u_1, u_2\}.
        \end{align*}
        <p align="right"> ◻</p>
    </div>


    <h6>Exercise 6</h6>
    Show that \eqref{eq:counter_monotonicity_copula} in <i>Definition 7</i> is
    indeed the copula associated with two random variables related by \(U_2=1-U_1\).
    <br>
    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 6</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
        Notice that
        \begin{align*}
        C(u_1, u_2) &= \mathbb{P}(U_1 \leq u_1, U_2 \leq u_2) \newline
        &= \mathbb{P}(U_1 \leq u_1, 1-U_1 \leq u_2) \newline
        &= \mathbb{P}(1 - u_2 \leq U_1 \leq u_1) \newline
        &= \max \{u_1 + u_2 - 1, 0\}.
        \end{align*}
        <p align="right"> ◻</p>
    </div>



    <hr class="w3-opacity">


As mentioned at the beginning of this section, these are extreme cases which
    are fundamental as they provide bounds to any other copula. This concept is formalised
    by the following theorem.

    <br><br>
    <b>Theorem 8 — Fréchet-Hoeffding bounds</b><blockquote><i> Given a copula
    \(C(\mathbf{u}) = C(u_1,..., u_d)\)
    one always has
    \begin{equation}
    W_{\text{counter}}(\mathbf{u})\le C(\mathbf{u}) \le C_{\text{co}}(\mathbf{u}).
    \label{eq:Frechet_Hoeffding_bounds}
    \end{equation}
</i></blockquote>
    <br><br>

Finally, let us consider the following proposition.

    <br><br>
    <b>Proposition 9</b><blockquote><i>
    Consider a random vector \(\mathbf{X}\) with copula \(C\),
    and let  \(T_1, ..., T_d\) be a set of \(d\) strictly increasing transformations.
    Then, the copula of the transformed vector \((T_1(X_1), ..., T_d(X_d))\) is also \(C\).

</i></blockquote>
    <br><br>


    <h2>Parametric families of copulae</h2>
The following table provides some examples of notable parametric families.
    <br><br>
    <table class="w3-table-all w3-hoverable">
        <tr>
            <th>Family</th>
            <th>CDF</th>
            <th>Parameter</th>
        </tr>
        <tr>
            <th colspan="4" style="text-align:center;">Elliptical copulae</th>
        </tr>
        <tr>
            <td>Gaussian</td>
            <td>\(\Phi_2(\Phi^{-1}(u_1),\ \Phi^{-1}(u_2);\ \rho)\)</td>
            <td>\(\rho \in [0,1] \)</td>
        </tr>
        <tr>
            <td>Student-t</td>
            <td>\(T_{2,\nu}\left(T_{1,\nu}^{-1}(u_1),\ T_{1,\nu}^{-1}(u_2);\ \rho\right)\)</td>
            <td>\(\rho \in [0,1],\ \nu\ge1 \)</td>
        </tr>
        <tr>
            <th colspan="4" style="text-align:center;">Archimedean copulae</th>
        </tr>
        <tr>
            <td>Clayton</td>
            <td>\( \max\left\{u_1^{-\theta}+u_2^{-\theta} - 1,\ 0\right\}^{-1/\theta} \)</td>
            <td>\(\theta \in [-1, \infty[ \backslash \{0\} \)</td>
        </tr>
        <tr>
            <td>Gumbel</td>
            <td>\( \exp \left( -\left( (-\log u_1)^{\theta} + (-\log u_2)^{\theta} \right)^{1/\theta} \right) \)</td>
            <td>\(\theta \in [1, \infty[ \)</td>
        </tr>
        <tr>
            <td>Joe</td>
            <td>\( 1 - \left( (1-u_1)^{\theta} + (1-u_2)^{\theta} - (1-u_1)^{\theta} (1-u_2)^{\theta} \right)^{1/\theta} \)</td>
            <td>\(\theta \in [1, \infty[ \)</td>
        </tr>
        <tr>
            <td>Frank</td>
            <td>\(-\theta^{-1} \log \left( \frac{1 - e^{-\theta} - (1 - e^{-\theta u_1}) (1 - e^{-\theta u_2})} {1-e^{-\theta}} \right) \)</td>
            <td>\(\theta\in\mathbb{R}\backslash \{0\}\)</td>
        </tr>

    </table>

    <br>

We can recognise two main classes of copulae: elliptical and archimedean.
    <i>Elliptical</i> copulae are implicit copulae arising from
    elliptical distributions
    (such as the normal, Student-t, and mixtures of normal distribution)
    via Sklar's theorem. Elliptical copulae have the advantage of allowing for
    flexible modelling of pairwise dependency, are simple to sample from usually densities are known.
    On the other hand, the copula itself is not explicit, and is characterised by
    radial symmetry, which implies identical lower and upper tail behaviour,
    as it we shall discuss.
    <br><br>
<i>Archimedean</i> copulae instead are explicit copulae which can be written
    in terms of some monotone generator
    \(\psi:[0,\infty[\rightarrow [0,1]\) such that \(\psi(0)=1\), \(\psi(\infty)=0\).
The associated archimedean copula is then written as
    \begin{equation}
    C(u_1, ..., u_d) = \psi\left(\sum_{i=1}^d\psi^{[-1]}(u_i)\right),
    \end{equation}
with the pseudo-inverse \(\psi^{[-1]}(x)=\psi^{-1}(x)\mathbb{1}_{x\le\psi(0)}\).
As an example, the generators of the archimedean copulae in the table above are:
    \begin{align*}
    \psi_{\text{Clayton}}(x) &= \theta^{-1}(x^{-\theta}-1),\newline
    \psi_{\text{Gumbel}}(x) &= (-\log x)^\theta,\newline
    \psi_{\text{Joe}}(x) &= -\log(1-(1-x)^\theta),\newline
    \psi_{\text{Frank}}(x) &= -\log\frac{e^{\theta x}-1}{e^{-\theta}-1}.
    \end{align*}

The explicit nature of Archimedean copulae makes them particularly suitable for
    closed form calculations, allowing for many of their properties to be
    explicitely computed in term of the generator \(\psi\). Moreover they are not
    anymore restricted to radial symmetries.

    <figure>
        <img class="center" alt="copulae_grid"
             src="img/copulae/grid_copulae.svg"
              width="80%" >
        <figcaption><i><b>Fig.1:</b> Bivariate probability density functions with normal marginals and selected copulae.</i></figcaption>
    </figure>

    <img class="center" src="img/L1/output_1_1.svg"  width="55%" >

    <img class="center" alt="copulae_grid"
         src="img/copulae/grid_copulae.svg"
         width="80%" >
    <button class="collapsible">Plot code</button>
    <div class="content"><pre><code class="lang-python">
import numpy as np, matplotlib.pyplot as plt, seaborn as sns
import scipy.stats as st
sns.set()

n = 1000
x1 = x2 = np.linspace(-2.7, 2.7, n)
dx = x1[-1] - x2[-2]
u1, u2 = st.norm.cdf(x1), st.norm.cdf(x2)

# INDEPENDENCE COPULA
def Ind_cop(u1, u2):
    U1, U2 = np.meshgrid(u1, u2)
    return U1 * U2

# GAUSSIAN COPULA
def Gauss_cop(u1, u2, rho=0.5):
    """Rho in [0, 1]"""
    iU1, iU2 = np.meshgrid(st.norm.ppf(u1), st.norm.ppf(u2))
    jnt = st.multivariate_normal.cdf(np.array([iU1, iU2]).T,
                                     mean=[0, 0], cov=[[1, rho], [rho, 1]])
    return jnt

# CLAYTON COPULA
def Clayton_cop(u1, u2, theta=1.):
    """Theta in [-1, inf[ \ {0}"""
    U1, U2 = np.meshgrid(u1, u2)
    jnt = (U1**(-theta) + U2**(-theta) - 1).clip(min=0) ** (-1/theta)
    return jnt

# GUMBEL COPULA
def Gumbel_cop(u1, u2, theta=1.):
    """Theta in [1, inf["""
    U1, U2 = np.meshgrid(u1, u2)
    jnt = np.exp(-((-np.log(U1))**theta
                   +(-np.log(U2))**theta)**(1/theta))
    return jnt

# JOE COPULA
def Joe_cop(u1, u2, theta=1.):
    """Theta in [1, inf["""
    U1, U2 = np.meshgrid(u1, u2)
    jnt = 1 - ((1-U1)**theta + (1-U2)**theta - ((1-U1)*(1-U2))**theta) ** (1/theta)
    return jnt

# FRANK COPULA
def Frank_cop(u1, u2, theta=1.):
    """Theta in [-inf, inf[ \ {0}"""
    U1, U2 = np.meshgrid(u1, u2)
    jnt = -(1/theta) * np.log(1+((np.exp(-theta*U1) - 1)*(np.exp(-theta*U2) - 1))/(np.exp(-theta) - 1))
    return jnt

J = Clayton_cop(u1, u2, theta=5)
dJ = (np.diff(np.diff(J).T).T).clip(min=1e-10)
plt.contour(x1[1:], x2[1:], dJ, 10)
plt.show()
</code></pre>
</div>



<br>
    <hr class="w3-opacity">

    <h6>Exercise 7</h6>
    Given a vector \((X_1, X_2)\) from the standard bivariate
    normal distribution with correlation \(\rho\), derive the gaussian copula
    presented in the table above.
    <br>
    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 7</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
        The associated copula is given by
        \begin{align*}
        C(u_1, u_2) &= \mathbb{P}(U_1 \leq u_1, U_2 \leq u_2) \newline
        &= \mathbb{P}(X_1 \leq \Phi^{-1}(u_1), X_2 \leq \Phi^{-1}(u_2)) \newline
        &= \Phi_2(\Phi^{-1}(u_1), \Phi^{-1}(u_2); \rho).
        \end{align*}
        <p align="right"> ◻</p>
    </div>

    <h6 id="EX8">Exercise 8</h6>
    Show that the limiting copula of the Clayton copula \(C_\theta^{\text{Clayton}}(u_1, u_2)\)
    for \(\theta\rightarrow\infty\) is the comonotonicity copula \(C_{\text{co}}\) and
    for \(\theta\rightarrow -1\) is the countermonotonicity copula \(W_{\text{counter}}\).
    Moreover show that for \(\theta\rightarrow 0\) Clayton's copula tends to the independence copula.
    <br>
    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 8</button>
    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">
        The case \(\theta\rightarrow -1\) is trivial.

        <p align="right"> ◻</p>

        Let's now consider the case \(\theta\rightarrow \infty\).
Assume \(u_1 \le u_2 \) in \([0,1]\). Then
        \[
u_1^{-\theta} \le \max\left\{u_1^{-\theta}+u_2^{-\theta} - 1,\ 0\right\} \le 2u_1^{-\theta}.
        \]
This in turns implies
        \[
2^{-1/\theta}u_1\le C_\theta^{\text{Clayton}}(u_1, u_2) \le u_1.
        \]
        Hence, one has
        \[
\lim_{\theta\rightarrow \infty}C_\theta^{\text{Clayton}}(u_1, u_2) = \text{min}\{u_1, u_2\}
        =C_{\text{co}}
        \]

        <p align="right"> ◻</p>

        Let's now consider the case \(\theta\rightarrow 0\). For \(u_1, u_2 \in [0,1]\) one has
        \[
u_j^{-\theta} = e^{-\theta\log u_j} \xrightarrow[]{\theta\rightarrow 0} 1-\theta\log u_j +o(\theta).
        \]
Hence
        \begin{align*}
\log C_\theta^{\text{Clayton}}(u_1, u_2)
        &= \frac{1}{\theta}\log(1-\theta\log(u_1u_2) + o(\theta)) \newline
        &\xrightarrow[]{\theta\rightarrow 0}\log(u_1u_2)
        \end{align*}

        <p align="right"> ◻</p>

    </div>

    <hr class="w3-opacity">

<!--    <h2>Dependence measures</h2>-->
<!--    For convenience, one often desires to summarise-->
<!--    the dependency structure of a multivariate distribution-->
<!--    with a single scalar metric. The most common measures is-->
<!--    linear correlation. However this metric is known to have its-->
<!--    severe pitfalls in many general cases, and better measures should be considered.-->

<!--    <h3>Linear correlation</h3>-->
<!--The first measure of dependency we encounter is Pearson's coefficient of-->
<!--    linear correlation:-->
<!--    \begin{equation}-->
<!--    \begin{split}-->
<!--    \rho(X_1, X_2) &= \frac{\text{cov}(X_1, X_2)}{\sigma_{X_1} \sigma_{X_2}} \\-->
<!--     &= \frac{\mathbb{E}((X_1-\mathbb{E}X_1)(X_2-\mathbb{E}X_2))}{\sqrt{\mathbb{E}((X_1-\mathbb{E}X_1)^2)}\sqrt{\mathbb{E}((X_2-\mathbb{E}X_2)^2)}}-->
<!--    \end{split}.-->
<!--    \label{eq:lin_corr}-->
<!--    \end{equation}-->
<!--While very useful in many linear or approximately linear scenarios, this metric however fails-->
<!--    to capture fundamental properties of more complex and realistic distributions.-->
<!--    In addition, thinking in terms of linear correlation can easily make us prey of insidious-->
<!--    pitfalls and fallacies.-->
<!--    <br>-->
<!--First of all we notice that \eqref{eq:lin_corr} depends on the marginals-->
<!--    and in particular, that it is well defined if and only if the second moments exist,-->
<!--    \(\mathbb{E}(X_j)^2<\infty,\ \forall j\). Therefore this metric is not well defined-->
<!--    for a number of marginals such as many power-law distributions where the second moment does not exist.-->
<!--    <br>-->
<!--    Moreover, the linear correlation \eqref{eq:lin_corr} is also unable to capture-->
<!--    strong non-linear functional dependencies such as \(X_2=X_1^2\) or \(X_2 = \sin(X_1)\).-->
<!--    Indeed in general one has \(|\rho|\le 1\) and \(|\rho|=1\iff X_2 = aX_1+b\)-->
<!--    for some \(a\in\mathbb{R}\backslash \{0\},\ b\in\mathbb{R} \).-->
<!--    <br>-->
<!--    The linear correlation \(\rho\) is also invariant under strictly increasing-->
<!--    <u>linear</u> transformation, but not under more general stricly increasing transformations.-->

<!--<br>-->
<!--    One further source of confusion arise from us being used to reason in terms of-->
<!--    normal distributions. Indeed a number of seemingly intuitive statements on-->
<!--    correlations which are true in the case of normal distributions do not generalise outside of this distribution.-->
<!--<br>-->
<!--    As an example, for normal distributions one finds zero correlation and independence equivalent, which-->
<!--    is no longer true already for e.g. student-t distributed random variables.-->
<!--<br>-->
<!--    Another fallacy is to think that the marginals and the correlations matrix-->
<!--    (\(F_1\), \(F_2\), and \(\rho\) in the bivariate case) are sufficient to determine-->
<!--    the joint distribution \(F\). This is true for elliptical distributions, but wrong in general.-->
<!--    Indeed the only mathematical object encoding all information concerning the dependency structure-->
<!--    is the copula itself.-->
<!--<br>-->
<!--    Yet another fallacy is to think two marginals \(F_1\), \(F_2\),-->
<!--    any value of \(\rho\in[-1,1]\) is attainable. Again, this is true for elliptically-->
<!--    distributed \((X_1, X_2)\) with finite second moment, but wrong in general.-->
<!--    The attainable range can be computed via <i>Hoeffding's formula</i>-->
<!--    \begin{equation}-->
<!--\text{cov}(X_1, X_2) = \int_{-\infty}^\infty\int_{-\infty}^\infty C(F_1(x_1), F_2(x_2))-F_1(x_1)F_2(x_2)\text{d} x \text{d} y,-->
<!--\label{eq:Hoeffding_formula}-->
<!--    \end{equation}-->
<!--where \(\rho_{\text{min}}\) is attained for \(C=W_{\text{counter}}\) and \(\rho_{\text{max}}\) for \(C=C_{\text{co}}\).-->
<!--    This can be arbitrarily small for appropriate choices of the marginals \(F_1\) and \(F_2\).-->
<!--    <hr class="w3-opacity">-->

<!--    <h6>Exercise 9</h6>-->
<!--    Consider two independent random variables \(Z, W \sim \mathcal{N}(0,1)\).-->
<!--    The random variables \(X=Z\) and \(Y = ZW\) are clearly not independent.-->
<!--    What's \(\rho(X, Y)\)?-->
<!--    <br>-->
<!--    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 9</button>-->
<!--    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">-->
<!--        The linear correlation coefficient is-->
<!--        \begin{align*}-->
<!--        \rho(X, Y) &= \text{cov}(X, Y) \newline-->
<!--        &= \mathbb{E}(XY)\newline-->
<!--        &= \mathbb{E}(V)\mathbb{E}(Z^2) = 0-->
<!--        \end{align*}-->
<!--        <p align="right"> ◻</p>-->
<!--    </div>-->

<!--    <h6>Exercise 10</h6>-->
<!--    Prove \eqref{eq:Hoeffding_formula}.-->
<!--    <br>-->
<!--    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 10</button>-->
<!--    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">-->
<!--        Start by considering an \(X_j, Y_j \sim F_j\),-->
<!--        with \(X_j \perp\!\!\!\perp Y_j\), for \(j\in\{1,2\}\).-->
<!--        One has:-->
<!--        \begin{align*}-->
<!--        2\text{cov}(X_1, X_2) &= \mathbb{E}((X_1-\mathbb{E}X_1)(X_2-\mathbb{E}X_2)) + \mathbb{E}((Y_1-\mathbb{E}Y_1)(Y_2-\mathbb{E}Y_2)) \newline-->
<!--        &=\mathbb{E}\left(((X_1 - \mathbb{E}X_1)-(Y_1 - \mathbb{E}Y_1))((X_2 - \mathbb{E}X_2)-(Y_2 - \mathbb{E}Y_2))\right) \newline-->
<!--        &= \mathbb{E}((X_1-Y_1)(X_2-Y_2)).-->
<!--        \end{align*}-->
<!--        Now recall that for any \(a,b\in\mathbb{R}\) one has-->
<!--        \[-->
<!--        b-a = \int_{-\infty}^\infty \Theta(x-a) - \Theta(x-b)\text{d}x,-->
<!--        \]-->
<!--        with \(\Theta(x)\) indicating Heaviside's theta function-->
<!--        (with the convention \(\Theta(0)=1\)). Therefore:-->
<!--        \begin{align*}-->
<!--        2\text{cov}(X_1, X_2) &=-->
<!--        \mathbb{E}\int_{-\infty}^\infty\int_{-\infty}^\infty-->
<!--(\Theta(x_1-Y_1) - \Theta(x_1-X_1))(\Theta(x_2-Y_2) - \Theta(x_2-X_2))-->
<!--        \text{d}x_1\text{d}x_2 \newline-->
<!--\xrightarrow[]{\text{Fubini}}&=\int_{-\infty}^\infty\int_{-\infty}^\infty-->
<!--        \mathbb{E}\left((\Theta(x_1-Y_1) - \Theta(x_1-X_1))(\Theta(x_2-Y_2) - \Theta(x_2-X_2))\right)-->
<!--        \text{d}x_1\text{d}x_2\newline-->
<!--        &=2\int_{-\infty}^\infty\int_{-\infty}^\infty F(x_1, x_2) - F_1(x_1)F_2(x_2)\text{d}x_1\text{d}x_2.-->
<!--        \end{align*}-->
<!--        <p align="right"> ◻</p>-->
<!--    </div>-->

<!--    <hr class="w3-opacity">-->

<!--    <h3>Rank correlation</h3>-->
<!--Many of the drawbacks and pitfalls encountered with linear correlation-->
<!--    are resolved when considering rank correlations instead.-->
<!--    As we shall see, rank correlation coefficients are always defined-->
<!--    and are invariant under any strictly increasing transformation, implying-->
<!--    they depend exclusively on the copula.-->
<!--    In the following we-->
<!--    shall present to of the most prominent, namely <i>Kendall's tau</i> and-->
<!--    <i>Spearman's rho</i>.-->
<!--    <br>-->



<!--    <br><br>-->
<!--    <b>Definition 10 — Kendall's tau</b><blockquote><i>-->
<!--    Consider \(X_j, Y_j \sim F_j\) for \(j \in \{1,2\}\).-->
<!--    Kendall's tau is defined as-->
<!--    \begin{equation}-->
<!--    \begin{split}-->
<!--    \rho_\tau &= \mathbb{E}(\text{sign}((X_1-Y_1)(X_2-Y_2))) \newline-->
<!--    &= \mathbb{P}((X_1-Y_1)(X_2-Y_2)>0) - \mathbb{P}((X_1-Y_1)(X_2-Y_2)<0).-->
<!--    \end{split}-->
<!--    \label{eq:kendall_tau}-->
<!--    \end{equation}-->
<!--    That is, the probability of concordance minus the probability of discordance-->
<!--    (i.e. the probability of two points from \(F\) to have positive or negative slope respectively).-->
<!--</i></blockquote>-->

<!--    <b>Proposition 10.1 — Formula for Kendall's tau</b><blockquote><i>-->
<!--    Consider two random variables \(X_1\) and \(X_2\)-->
<!--    with marginals \(F_1\) and \(F_2\) and copula \(C\). One has:-->
<!--    \begin{equation}-->
<!--    \begin{split}-->
<!--    \rho_\tau &= 4\int_{[0,1]^2}C(u_1, u_2)\text{d}C(u_1, u_2) - 1\newline-->
<!--    &=4\mathbb{E}(C(U_1, U_2)) - 1,-->
<!--    \end{split}-->
<!--    \label{eq:formula_kendall}-->
<!--    \end{equation}-->
<!--    with \((U_1, U_2)\sim C\).-->

<!--</i></blockquote>-->


<!--    <br>-->
<!--    <b>Definition 11 — Spearman's rho</b><blockquote><i>-->
<!--    Consider \(X_j, Y_j \sim F_j\) for \(j \in \{1,2\}\).-->
<!--    Spearman's rho is defined as-->
<!--    \begin{equation}-->
<!--    \rho_S = \rho(F_1(X_1), F_2(X_2)).-->
<!--    \label{eq:spearman_rho}-->
<!--    \end{equation}-->
<!--</i></blockquote>-->


<!--    <b>Proposition 11.1 — Formula for Spearman's rho</b><blockquote><i>-->
<!--    Consider two random variables \(X_1\) and \(X_2\)-->
<!--    with marginals \(F_1\) and \(F_2\) and copula \(C\). One has:-->
<!--    \begin{equation}-->
<!--    \begin{split}-->
<!--    \rho_S &=12 \int_0^1\int_0^1 C(u_1, u_2)\text{d}u_1\text{d}u_2 - 3 \newline-->
<!--    &= 12\mathbb{E}(C(U_1, U_2)) - 3,-->
<!--    \end{split}-->
<!--    \label{eq:formula_spearman}-->
<!--    \end{equation}-->
<!--    with \(U_1 \perp\!\!\!\perp U_2\).-->
<!--</i></blockquote>-->


<!--    Rank correlations are useful to characterise the dependence, providing comparable measures,-->
<!--    and can also serve as a tool for calibration and estimation of a copula's parameter(s).-->
<!--    As we have mentioned, a number of fallacies are avoided, but not all.-->
<!--    <h5>Resolved fallacies</h5>-->
<!--    For \(\kappa=\rho_\tau\) as well as for  \(\kappa=\rho_S\) one has:-->
<!--    <ul>-->
<!--        <li>\(\kappa\) is always well defined</li>-->
<!--        <li>\(\kappa\) is invariant under any strictly increasing transformation of the random variables</li>-->
<!--        <li>\(\kappa=\pm 1\) if and only if \(X_1\) and \(X_2\) are co-/counter- monotonic.</li>-->
<!--        <li>Any \(\kappa \in [-1,1]\) is attainable</li>-->
<!--    </ul>-->
<!--    <h5>Unresolved fallacies</h5>-->
<!--    However, in general for \(\kappa=\rho_\tau\) as well as for  \(\kappa=\rho_S\) one has:-->
<!--    <ul>-->
<!--        <li>The marginals \(F_1, F_2\) and the rank correlation \(\kappa\) are still not sufficient-->
<!--        to uniquely determine \(F\).</li>-->
<!--        <li> While \(X_1 \perp\!\!\!\perp X_2 \implies \kappa=0\), the converse is still not true: \(\kappa=0\) does not imply independence </li>-->
<!--    </ul>-->

<!--The last point is something that might still be desirable.-->
<!--    However one can show that this requirement would be be in contraddiction with the-->
<!--    fundamental property of invariance under strictly increasing transformations.-->

<!--    <br><br>-->
<!--    <b>Proposition 12</b><blockquote><i>-->
<!--    There exist no dependency measure \(\kappa\) such that:-->
<!--    <ol>-->
<!--        <li>\(\kappa(X_1, X_2) \iff X_1 \perp\!\!\!\perp X_2\), and</li>-->
<!--        <li>\(\kappa(T(X_1), X_2) = \begin{cases}-->
<!--            \kappa(X, Y) & \text{if $T$ strictly increasing}\\-->
<!--            -\kappa(X, Y) & \text{if $T$ strictly decreasing}-->
<!--            \end{cases}  \) .</li>-->
<!--    </ol>-->
<!--</i></blockquote>-->
<!--    See <a href="#EX16"><i>Exercise 16</i></a> for a proof. Nonetheless it is still possible to define a dependency measure \(\kappa\) such that-->
<!--    \(\kappa(X_1, X_2) \iff X_1 \perp\!\!\!\perp X_2\) as long as one is willing to trade off-->
<!--    other properties. In particular, it can be shown-->
<!--    that one can have \(\kappa(X_1, X_2) \iff X_1 \perp\!\!\!\perp X_2\),-->
<!--    with \(0\le \kappa(X_1, X_2)\le 1\), and-->
<!--    \(\kappa(X_1, X_2)=1 \iff X_1,X_2\) co-/counter- monotonic,-->
<!--    and \(\kappa(T(X_1), X_2)=\kappa(X_1, X_2)\) for \(T\) strictly increasing.-->
<!--    See <a href="#REFERENCES">[3]</a> for more details.-->
<!--    <hr class="w3-opacity">-->

<!--    <h6>Exercise 11</h6>-->
<!--    Prove \eqref{eq:formula_kendall}.-->
<!--    <br>-->
<!--    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 11</button>-->
<!--    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">-->
<!--        Consider \(X_j, Y_j \sim F_j\),-->
<!--        with \(X_j \perp\!\!\!\perp Y_j\), for \(j\in\{1,2\}\).-->
<!--        One has:-->
<!--        \begin{align*}-->
<!--        \rho_\tau &= \mathbb{P}((X_1-Y_1)(X_2-Y_2)>0) - \mathbb{P}((X_1-Y_1)(X_2-Y_2)<0) \newline-->
<!--        &= 2\mathbb{P}((X_1-Y_1)(X_2-Y_2)>0) - 1 \newline-->
<!--        &= 2(2\mathbb{P}(X_1< Y_1,\ X_2< Y_2)) -1 \newline-->
<!--        &= 4\mathbb{P}(U_{X,1}< U_{Y,1},\ U_{X,2}< U_{Y,2}) -1 \newline-->
<!--        &=4\int_0^1\int_0^1\mathbb{P}(U_1\le u_1, U_2\le u_2)\text{d}C(u_1, u_2) - 1.-->
<!--        \end{align*}-->
<!--        <p align="right"> ◻</p>-->


<!--    </div>-->
<!--    <h6>Exercise 12</h6>-->
<!--    Prove \eqref{eq:formula_spearman}.-->
<!--    <br>-->
<!--    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 12</button>-->
<!--    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">-->
<!--From definition \eqref{eq:spearman_rho} one has:-->
<!--        \begin{align*}-->
<!--        \rho_S &= \rho(F_1(X_1), F_2(X_2)) \newline-->
<!--        \text{Hoeffding's formula}\rightarrow &= 12\int_0^1\int_0^1C(u_1, u_2)-u_1u_2\ \text{d}u_1\text{d}u_2 \newline-->
<!--        &=12\int_0^1\int_0^1C(u_1, u_2) \text{d}u_1\text{d}u_2 -3.-->
<!--        \end{align*}-->
<!--        <p align="right"> ◻</p>-->
<!--    </div>-->
<!--    <h6>Exercise 13</h6>-->
<!--    Show that the comonotonic copula \(C_{\text{co}}\) implies \(\kappa=1\)-->
<!--    for both \(\kappa=\rho_\tau\) and \(\kappa=\rho_S\).-->
<!--    <br>-->
<!--    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 13</button>-->
<!--    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">-->
<!--        Notice that-->
<!--        \[-->
<!--        \mathbb{P}(\text{min}(U_1, U_2) < t) = 1-\mathbb{P}(\text{min}(U_1, U_2) \ge t) = 1- (1-t)^2,-->
<!--        \]-->
<!--        so that-->
<!--        \(-->
<!--        f_{T=\text{min}(U_1, U_2)}(t) = 2(1-t).-->
<!--        \)-->
<!--        Therefore one has-->
<!--        \begin{align*}-->
<!--        \mathbb{E}(\text{min}(U_1, U_2)) &= \int_0^1 tf_{T=\text{min}(U_1, U_2)}(t) \text{d}t \newline-->
<!--        &= 2\int_0^1 t(1-t) \text{d}t =\frac{1}{3}.-->
<!--        \end{align*}-->
<!--        Hence:-->
<!--                \begin{align*}-->
<!--&lt;!&ndash;                \rho_\tau &= 4\mathbb{E}(\text{min}(U_1, U_2)) - 1 = 4\frac{}{},\newline&ndash;&gt;-->
<!--                \rho_S &= 12\mathbb{E}(\text{min}(U_1, U_2)) - 3 = \frac{12}{3} - 3 = 1.-->
<!--                \end{align*}-->
<!--        Let's consider Kendall's tau now. First, the copula viewed as a random variable-->
<!--        has a distribution function called "Kendall distribution function", which is equal to-->
<!--        \[-->
<!--K_C(t) = t -\frac{\psi(t)}{\psi'(t)},-->
<!--        \]-->
<!--with \(\psi(t)\) the copula's generator and \(\psi'(t)\) its first derivative.-->
<!--One can then write the expected value of the copula as-->
<!--        \begin{align*}-->
<!--\mathbb{E}[C(U_1, U_2)] &= \int_0^1t\text{d}K_C \newline-->
<!--      \xrightarrow[]{\text{by parts}}  &= tK_C(t)\Big|_0^1 - \int_0^1K_C(t)\text{d}t.-->
<!--        \end{align*}-->


<!--        With the results from  <i>Exercise 8</i> in mind, let's consider the generator of-->
<!--        the Clayton copula and its derivative:-->
<!--        \begin{align*}-->
<!--\psi(t) &= \theta^{-1}(t^{-\theta}-1), \newline-->
<!--        \psi'(t) &= -t^{-(1+\theta)},-->
<!--        \end{align*}-->
<!--which gives-->
<!--        \[-->
<!--K_C(t) = t(1+\theta^{-1}(1-t^\theta)).-->
<!--        \]-->
<!--        Then-->
<!--        \begin{align*}-->
<!--        \mathbb{E}[C(U_1, U_2)] &= tK_C(t)\Big|_0^1 - \int_0^1K_C(t)\text{d}t\newline-->
<!--        &= 1-\frac{\theta+3}{2\theta+4}.-->
<!--        \end{align*}-->

<!--Finally, taking the limits \(\theta\rightarrow\infty\) for \(C_{\text{co}}\) and-->
<!--        \(\theta\rightarrow -1\) for \(W_{\text{counter}}\) one finds:-->

<!--        \begin{align*}-->
<!--        \mathbb{E}[C_{\text{co}}(U_1, U_2)] &= \frac{1}{2},\newline-->
<!--        \mathbb{E}[W_{\text{counter}}(U_1, U_2)] &= 0.\newline-->
<!--        \end{align*}-->
<!--Therefore,-->
<!--        \begin{align*}-->
<!--        \rho_\tau &= 4\mathbb{E}[C_{\text{co}}(U_1, U_2)]-1 = 1.-->
<!--        \end{align*}-->
<!--        <p align="right"> ◻</p>-->
<!--    </div>-->

<!--    <h6>Exercise 14</h6>-->
<!--    Show that the countermonotonic copula \(W_{\text{counter}}\) implies \(\kappa=-1\)-->
<!--    for both \(\kappa=\rho_\tau\) and \(\kappa=\rho_S\).-->
<!--    <br>-->
<!--    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 14</button>-->
<!--    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">-->
<!--        As in the solution of <i>Exercise 13</i> and with the results therein.-->
<!--        <p align="right"> ◻</p>-->
<!--    </div>-->

<!--    <h6>Exercise 15</h6>-->
<!--    Write a bivariate joint distribution parametrised by a single parameter \(\lambda\in [0,1]\)-->
<!--    and show that this can attain any \(\kappa \in [-1,1]\) for both \(\kappa=\rho_\tau\) and \(\kappa=\rho_S\).-->
<!--    <br>-->
<!--    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 15</button>-->
<!--    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">-->
<!--        Consider the following:-->
<!--        \begin{equation*}-->
<!--        F(x_1, x_2) = \lambda C_{\text{co}}(F_1(x_1), F_2(x_2)) + (1-\lambda)W_{\text{counter}}(F_1(x_1), F_2(x_2)).-->
<!--        \end{equation*}-->
<!--Therefore one has-->
<!--        \[-->
<!--\kappa = \lambda - (1-\lambda) = 2\lambda -1.-->
<!--        \]-->
<!--        That is, in general one has \(\rho_\tau=\rho_S=2\lambda-1\).-->
<!--        <p align="right"> ◻</p>-->
<!--    </div>-->

<!--    <h6 id="EX16">Exercise 16</h6>-->
<!--    Prove <i>Proposition 8</i>.-->
<!--    <br>-->
<!--    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 16</button>-->
<!--    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">-->
<!--        Consider \((X_1, X_2)\) uniformely distributed on the unit circle in \(\mathbb{R}^2\),-->
<!--        so that the vector can be parametrised by \(\phi\sim \mathcal{U}[0,2\pi]\)-->
<!--        as \((X_1, X_2)=(\cos\phi, \sin\phi)\).-->
<!--        Because \((X_1, X_2) \stackrel{\text{d}}{=} (-X_1, X_2)\) one has-->
<!--        \begin{equation*}-->
<!--        \kappa(-X_1, X_2) = \kappa(X_1, X_2) = -\kappa(X_1, X_2).-->
<!--        \end{equation*}-->
<!--        This implies \(\kappa(X_1, X_2)=0\) although \(X_1\) and  \(X_2\) are not independent,-->
<!--        which is a contraddiction.-->
<!--        See also <a href="#REFERENCES">[3]</a>.-->
<!--        <p align="right"> ◻</p>-->
<!--    </div>-->

<!--    <hr class="w3-opacity">-->

<!--    <h3>Coefficients of tail dependence</h3>-->

<!--    If one wants to study extreme values, asymptotic measures of tail dependence-->
<!--    can be defined as a function of the copula. In what follows we shall-->
<!--    distinguish between <i>upper</i> tail dependence and <i>lower</i> tail dependence.-->

<!--    <br><br>-->
<!--    <b>Definition 13 — Coefficient of tail dependence</b><blockquote><i>-->
<!--    Consider two random variables \(X_j\sim F_j\).-->
<!--    The associated coefficients of upper and lower tail dependence are:-->
<!--    \begin{equation}-->
<!--    \begin{split}-->
<!--    \lambda_{u} &= \lim_{\alpha\rightarrow 1^-}\mathbb{P}(X_2> F_2^{-1}(\alpha)|X_2 > F_1^{-1}(\alpha)),\newline-->
<!--    \lambda_{\ell} &= \lim_{\alpha\rightarrow 0^+}\mathbb{P}(X_2\le F_2^{-1}(\alpha)|X_2\le F_1^{-1}(\alpha)).-->
<!--    \end{split}-->
<!--    \end{equation}-->
<!--If \(\lambda_{u}\in]0,1]\) (\(\lambda_{\ell}\in]0,1]\)), then \((X_1, X_2)\)-->
<!--    is said to be upper (lower) tail dependent, or more generally, asymptotically dependent. Similarly,-->
<!--    if \(\lambda_{u}=0\) (\(\lambda_{\ell}=0\)), then \((X_1, X_2)\)-->
<!--    is said to be upper (lower) tail independent, or more generally,  asymptotically independent.-->
<!--</i></blockquote>-->
<!--    <b>Proposition 13.1</b><blockquote><i>-->
<!--    The coefficients of upper and lower tail dependence can be written as a function-->
<!--    of the copula as:-->
<!--    \begin{equation}-->
<!--    \begin{split}-->
<!--    \lambda_{u} &= \lim_{\alpha\rightarrow 1^-}2-\frac{1-C(\alpha, \alpha)}{1-\alpha},\newline-->
<!--    \lambda_{\ell} &= \lim_{\alpha\rightarrow 0^+}\frac{C(\alpha,\alpha)}{\alpha}.-->
<!--    \end{split}-->
<!--    \end{equation}-->
<!--</i></blockquote>-->
<!--    <b>Proposition 13.2</b><blockquote><i>-->
<!--    For radially symmetric copulae one has \(\lambda_{u}=\lambda_{\ell}\).-->
<!--</i></blockquote>-->
<!--    <b>Proposition 13.3</b><blockquote><i>-->
<!--For Archimedean copulae with strict generator \(\psi\) one has-->
<!--    \begin{equation}-->
<!--    \begin{split}-->
<!--    \lambda_{u} &= 2-2\lim_{\alpha\rightarrow 0^+}\frac{\psi'(2\alpha)}{\psi'(\alpha)},\newline-->
<!--    \lambda_{\ell} &= 2\lim_{\alpha\rightarrow\infty}\frac{\psi'(2\alpha)}{\psi'(\alpha)}.-->
<!--    \end{split}-->
<!--    \end{equation}-->
<!--</i></blockquote>-->

<!--    <hr class="w3-opacity">-->

<!--    <h6>Exercise 17</h6>-->
<!--    Prove <i>Proposition 13.1</i>.-->
<!--    <br>-->
<!--    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 17</button>-->
<!--    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">-->
<!--        One has:-->
<!--        \begin{align*}-->
<!--        \lambda_{u} &= \lim_{\alpha\rightarrow 1^-}\mathbb{P}(U_2> \alpha |U_1 > \alpha) \newline-->
<!--        &=\lim_{\alpha\rightarrow 1^-}\frac{1- \mathbb{P}(U_2\le \alpha\ \text{or}\ U_1\le \alpha)}{\mathbb{P}(U_1> \alpha)} \newline-->
<!--        &=\lim_{\alpha\rightarrow 1^-}\frac{1- \mathbb{P}(U_2\le \alpha)-\mathbb{P}(U_1\le \alpha) + \mathbb{P}(U_2\le \alpha, U_1\le \alpha)}{1- \alpha}\newline-->
<!--        &=\lim_{\alpha\rightarrow 1^-}2-\frac{1-C(\alpha, \alpha)}{1-\alpha}.-->
<!--        \end{align*}-->
<!--        Similarly,-->
<!--        \begin{align*}-->
<!--        \lambda_{\ell} &= \lim_{\alpha\rightarrow 0^+}\mathbb{P}(U_2\le \alpha |U_1 \le \alpha) \newline-->
<!--        &=\lim_{\alpha\rightarrow 0^+}\frac{\mathbb{P}(U_2\le \alpha, U_1\le \alpha)}{\mathbb{P}(U_1\le \alpha)} \newline-->
<!--        &=\lim_{\alpha\rightarrow 0^+}\frac{C(\alpha,\alpha)}{\alpha}.-->
<!--        \end{align*}-->
<!--        <p align="right"> ◻</p>-->
<!--    </div>-->

<!--    <h6>Exercise 18</h6>-->
<!--    Prove <i>Proposition 13.3</i>. Moreover, compute the upper and lower coefficients-->
<!--    for Clayton's and Gumbel's copulae.-->
<!--    <br>-->
<!--    <button class="collapsible" style="margin-bottom: 10px;margin-top: 10px">Solution 18</button>-->
<!--    <div class="content" style="background-color: rgba(85,203,253,0.12) ; padding: 20px; border: 2px solid #9cc5ff;">-->
<!--        Consider the upper coefficient first. One has:-->
<!--        \begin{align*}-->
<!--        \lambda_u &= 2-\lim_{\alpha\rightarrow 1^-}\frac{1-\psi(2\psi^{-1}(\alpha))}{1-\alpha} \newline-->
<!--        \xrightarrow[]{\beta=\psi(\alpha)}&= 2-\lim_{\beta\rightarrow 0^+}\frac{1-\psi(2\beta)}{1-\psi(\beta)}\newline-->
<!--        \xrightarrow[]{\text{de l'Hôspital}}&=2-2\lim_{\beta\rightarrow 0^+}\frac{\psi'(2\beta)}{\psi'(\beta)}.-->
<!--        \end{align*}-->
<!--        Now the lower coefficient:-->
<!--        \begin{align*}-->
<!--\lambda_\ell &= \lim_{\alpha\rightarrow 0^+}\frac{\psi(2\psi^{-1}(\alpha))}{\alpha} \newline-->
<!--        \xrightarrow[]{\beta=\psi(\alpha)}&= \lim_{\beta\rightarrow\infty}\frac{\psi(2\beta)}{\psi(\beta)}\newline-->
<!--        \xrightarrow[]{\text{de l'Hôspital}}&=2\lim_{\beta\rightarrow\infty}\frac{\psi'(2\beta)}{\psi'(\beta)}.-->
<!--        \end{align*}-->

<!--        Finally, for the Clayton copula one finds:-->
<!--        \begin{align*}-->
<!--        \lambda_u &= 0,\newline-->
<!--        \lambda_\ell &= 2^{-1/\theta},-->
<!--        \end{align*}-->
<!--        while for the Gumbel copula one has:-->
<!--        \begin{align*}-->
<!--        \lambda_u &= 2-2^{1/\theta},\newline-->
<!--        \lambda_\ell &= 0.-->
<!--        \end{align*}-->
<!--        <p align="right"> ◻</p>-->
<!--    </div>-->

<!--    <hr class="w3-opacity">-->





<!--    As a digression, the 2007–2008 financial crisis is partially caused by the misuse of the Gaussian copula model. See Salmon (2009) and Donnelly and Embrechts (2010) for details. One of the main disadvantages of the model is that it does not adequately model the occurrence of defaults in the underlying portfolio of corporate bonds. In times of crisis, corporate defaults occur in clusters, so that if one company defaults then it is likely that other companies also default within a short period. However, under the Gaussian copula model, company defaults become independent as their size of default increases, which leads to model inadequacy.-->







    <br>

    <h4 id="REFERENCES"><i>References</i></h4>

    [1] <a href="https://press.princeton.edu/books/hardcover/9780691166278/quantitative-risk-management">"Quantitative Risk Management: Concepts, Techniques and Tools", Alexander J. McNeil, Paul Embrechts, and Rüdiger Frey (2005)</a>
    <br>
    [2]<a href="https://www.routledge.com/Dependence-Modeling-with-Copulas/Joe/p/book/9781466583221"> "Dependence Modeling with Copulas", Harry Joe (2015) </a>
    <br>
    [3]<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.321.5607&rep=rep1&type=pdf"> "Correlation and Dependence in Risk Management: Properties and Pitfalls", Paul Embrechts, Alexander McNeil, and Daniel Straumann, 1999</a>

    <br><br>


    <!-- ________________________________________________________________________________________________ -->
    <!-- ________________________________________________________________________________________________ -->
    <!-- ________________________________________________________________________________________________ -->
<!--    <h7>-->
        <!-- <span style="float:right;">
       <a href="./L2.html"><b><img src="../Icons/next.png" width="20px" align="right">Next</b></a>
       </span>
       </h7>
       <br> -->

        <h5> <a href="../Teaching.html"><b><img src="../Icons/back.png" width="20px"> Back to Teaching</b></a></h5>



        <hr class="w3-opacity">
</div>
</div>
<script type="text/javascript" src="../js/footer.js"></script>

<!-- End page content -->
</div>

<script src="../js/OpenCloseSidebar.js"></script>
<!--<script src="../js/ModalImageGallery.js"></script>-->
<script src="../js/OpenCloseCollapsible.js"></script>

</body>
</html>
